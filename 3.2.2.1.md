# 商品分类

> 商品表有两个字段：行业商品名称 ，product_category_first_name <br>
> 喂给模型商品名称，希望模型能预测出来正确的商品分类 <br>
> 数据集来源：https://aistudio.baidu.com/aistudio/datasetdetail/135131 <br>

## 一、准备数据

```python
# 引入类库
import sys

import paddle
import numpy as np
import matplotlib.pyplot as plt
import paddle.nn as nn
import os
import pandas as pd
import numpy as np
import time
import paddle.nn.functional as F
import paddle.nn.initializer as I
import pandas 
import numpy

# 查看当前版本
print(paddle.__version__) 
# cpu/gpu环境选择，在 paddle.set_device() 输入对应运行设备。
use_gpu = True if paddle.get_device().startswith("gpu") else False
if use_gpu:
    paddle.set_device('gpu:0')
    print("Use gpu 0")
else:
    print('none')
# paddle.set_device('gpu:0')
# device = paddle.set_device('gpu')  #使用gpu
device = paddle.set_device('cpu')  #使用cpu

# 数据集预处理
# 从原始目录data，经过处理复制到work/goods目录下

data_root_path = 'work/goods/'  # 数据集根目录
# data_file = 'news_classify_data.txt'  # 数据集名称

# 这些文件都是要生成的
data_file = 'data.txt'  # 数据集名称
test_file = 'Test_IDs.txt'  # 测试集
valid_file = 'Val_IDs.txt'  # 测试集
train_file = 'Train_IDs.txt'  # 训练集
dict_file = 'dict.txt'  # 编码生成的字典
# 保存模型
model_save_dir = 'model/'

# 读取完整的数据集
df=pd.read_csv("data/data135131/standard_product_library_535543_20220331.csv")[["行业商品名称","product_category_first_name"]]
# 剔除分类为空的记录 
print(df.head())
print("清理前:",df.shape[0])
df=df[~df["product_category_first_name"].isnull()]
df=df[~df["行业商品名称"].isnull()]
df["行业商品名称"]=df["行业商品名称"].map(lambda x: x.replace("\n","")) # 替换中间的换行符
print("清理后:",df.shape[0])
label_list=df["product_category_first_name"].unique().tolist()

# label_list=np.array(label_list)
# print(label_list)
# 分类器
class_dim=len(label_list)
print("品类总数：", (class_dim))

print(label_list)
# 把label信息存入磁盘
fileName= os.path.join(data_root_path,"labels.txt") 
with open(fileName,'w')as file:
    # for row in label_list:
    #     file.write("\n".join(row))
    file.write("\n".join(label_list))
    



# 从csv数据表生成字典
def make_kind_data(df):
    i=0
    txt=""
    for index,row in df.iterrows():
        title=row["行业商品名称"]
        catelogy=row["product_category_first_name"]
        # print(catelogy)
        txt=txt+"{}\t{}\t{}".format(label_list.index(catelogy),catelogy,title)+"\r"
        i=i+1

    data_file_path = os.path.join(data_root_path, data_file)
    # data_file_path="work/bomkind/data.txt"
    print(data_file_path)
    with open(data_file_path, 'w', encoding='utf-8') as f:
          f.write(str(txt))  # 将字典内容转换为字符串写入文件


def create_dict(data_file_path, dict_file_path):
    """
        把每个汉字编码成一个数字,并存入字典
    :param data_file_path: 数据存放路径
    :param dict_file_path: 字典存放路径
    :return:
    """
    dict_set = set()  # 集合,用来去除重复的汉字
    with open(data_file_path, encoding='utf-8') as f:
        lines = f.readlines()
    # 遍历每一行,循环处理
    for line in lines:
        # title = line.split('_!_')[-1].replace('\n', '')
        title = line.split('\t')[-1].replace('\n', '')
        for w in title:  # 取出标题中的每个字,添加到集合当中
            dict_set.add(w)

    dict_list = []
    i = 0
    for s in dict_set:
        dict_list.append((s, i))  # 将 字-编码 映射存入列表中
        i += 1
    # 将列表转换为字典,并且添加一个未知字符
    dict_txt = dict(dict_list)  # 将列表转换为字典
    end_dict = {'<unk>': i}  # 添加未知字符,用于编码不认识的字
    dict_txt.update(end_dict)  # 将未知字符更新到字典中
    #  将字典内容保存到文件中
    with open(dict_file_path, 'w', encoding='utf-8') as f:
        f.write(str(dict_txt))  # 将字典内容转换为字符串写入文件

    print('生成字典完成.')


# 数据标记:对每行标题编码,并且分测试集和训练集
def create_data_list():
    # 清空测试集,验证集,训练集三个文件
    test_file_path = os.path.join(data_root_path, "Test.txt")  # test_file
    with open(test_file_path, 'w') as f1:
        pass

    test_file_path_ids = os.path.join(data_root_path, "Test_IDs.txt")
    with open(test_file_path_ids, 'w') as f1:
        pass

    valid_file_path = os.path.join(data_root_path, valid_file)
    with open(valid_file_path, 'w') as f1:
        pass

    train_file_path = os.path.join(data_root_path, train_file)
    with open(train_file_path, 'w') as f2:
        pass

    # 从文件中读入字典数据
    dict_file_path = os.path.join(data_root_path, dict_file)
    with open(dict_file_path, encoding='utf-8') as f_dict:
        dict_txt = eval(f_dict.readlines()[0])  # 由文件生成字典

    # 读入原始的样本数据
    news_file_path = os.path.join(data_root_path, data_file)
    with open(news_file_path, encoding='utf-8') as f_data:
        lines = f_data.readlines()

    # 将文章中的标题提取出来,每个字转换为对应的编码
    # 形成一行新的数据,存到新的文件
    i = 0
    for line in lines:
        # words = line.replace('\n', '').split('_!_')
        words = line.split('\t')
        label = words[0]  # 类别
        title = words[-1]  # 标题
        if not i % 5:  # 划分测试集,20%  ，求余数  0,5,10,15 %5 =False ，求反，得正
            if i % 2:
                with open(valid_file_path, 'a', encoding='utf-8') as f_valid:
                    new_line = line_encode(title, dict_txt, label)
                    f_valid.write(new_line)
            else:
                # with open(test_file_path, 'a', encoding='utf-8') as f_test:
                #     new_line = line_encode(title, dict_txt, label)
                #     f_test.write(new_line)

                with open(test_file_path, 'a', encoding='utf-8') as f_test:
                    # new_line = line_encode(title, dict_txt, "") # label
                    new_line = title
                    f_test.write(new_line)

                with open(test_file_path_ids, 'a', encoding='utf-8') as f_test:
                    new_line = line_encode(title, dict_txt, "").replace("\t","") # 不用label
                    f_test.write(new_line)


        else:  # 划分训练集 i % 5 ，相当于取80%记录，除以5，有余数，占比大多数
            with open(train_file_path, 'a', encoding='utf-8') as f_train:
                new_line = line_encode(title, dict_txt, label)
                f_train.write(new_line)
        i += 1
    print('成功生成训练集,测试集')

def line_encode(title, dict_txt, label):
    """
        将新闻标题编码为一串数字
    :return: new_line
    """
    new_line = ''
    for w in title:
        if w in dict_txt:  # 字在字典中已经有了编码
            code = str(dict_txt[w])  # 从字典中取出字的编码值
        else:
            code = str(dict_txt['<unk>'])
        new_line = new_line + code + ','  # 将编码追加到新串
    new_line = new_line[:-1]  # 把最后一个逗号去掉
    new_line = new_line + '\t' + label + '\n'
    return new_line

# 初始化文件
# if False:
if True:
    # 从excel数据表生成字典
    make_kind_data(df)
    # 把每个汉字编码成一个数字, 并存入字典
    create_dict(data_root_path + data_file, data_root_path + dict_file)
    # 数据标记:对每行标题编码,并且分测试集，验证集和训练集，占比: 2/2/6
    create_data_list()

# 字典读取
def get_dict_len(d_path):
    with open(d_path, 'r', encoding='utf-8') as f:
        line = eval(f.readlines()[0])
    return line

word_dict = get_dict_len( os.path.join(data_root_path,  'dict.txt'))

# 准备数据集
# 训练集和验证集读取
set = []
def dataset(datapath):  # 数据集读取代码
    with open(datapath)as f:
        for i in f.readlines():
            data = []
            #查找'\t'最后一次出现的位置索引，以逗号分隔，得到
            dataset = i[:i.rfind('\t')].split(',')  # 获取文字内容
            dataset = np.array(dataset)
            data.append(dataset)
            label = np.array(i[i.rfind('\t')+1:-1])  # 获取标签
            data.append(label)
            set.append(data)
    return set

#标题ID+标签ID
train_dataset = dataset(os.path.join(data_root_path,'Train_IDs.txt'))
val_dataset = dataset(os.path.join(data_root_path,'Val_IDs.txt'))
# test_data = dataset('work/bomkind/Test_IDs.txt')  # 读取数据

print("训练数据集:")
print(train_dataset[:10])

print("验证数据集:")
print(val_dataset[0:10])


# 数据初始化
# 初始数据准备
vocab_size = len(word_dict) + 1  # 字典长度加1
emb_size = 256  # 神经网络长度
seq_len = 200  # 默认30，最长512 数据集长度（需要扩充的长度） 目标序列长度，该值需要预先对文本长度进行分别得到，可以设置为小于等于512（BERT的最长文本序列长度为512）的整数。
batch_size = 32  # 批处理大小
epochs = 50  # 训练轮数
pad_id = word_dict['<unk>']  # 空的填充内容值

# 生成句子列表（数据码生成文本）
def ids_to_str(ids):
    # print(ids)
    words = []
    for k in ids:
        w = list(word_dict)[eval(k)]
        words.append(w if isinstance(w, str) else w.decode('ASCII'))
    return " ".join(words)

print(train_dataset[1])

# 查看数据内容
for i in  train_dataset:
    sent = i[0]
    label = int(i[1])
    print('sentence list id is:', sent)  # 数据内容
    print('sentence label id is:', label)  # 对应标签
    print('--------------------------')  # 分隔线
    print('sentence list is: ', ids_to_str(sent))  # 转换后的数据
    print('sentence label is: ', label_list[label])  # 转换后的标签
    break


# 数据扩充并查看【不满max_len(30)的填充unk】
def create_padded_dataset(dataset):
    padded_sents = []
    labels = []
    for batch_id, data in enumerate(dataset):  # 读取数据
        sent, label = data[0], data[1]  # 标签和数据拆分
        # print(sent,"|",label)
        str1=sent[:seq_len]
        # print("st1:",str1)
        add_len=seq_len - len(sent)
        str2=[pad_id] *add_len
        # print("st2:",str2)
        if add_len>0: # 填充空值
            str3=np.concatenate([str1, str2])
        else:
            str3=str1 
        # print("st3:",str3)
        # padded_sent = np.concatenate([sent[:seq_len], [pad_id] * (seq_len - len(sent))]).astype('int32')  # 数据拼接

        if len(str1[0])>0:  # 避免空值
            # print(batch_id,str3)
            padded_sent = str3.astype('int32')  # 数据拼接
            # print(padded_sent)
            padded_sents.append(padded_sent)  # 写入数据
            labels.append(label)  # 写入标签
    # print(padded_sents)
    return np.array(padded_sents), np.array(labels).astype('int64')  # 转换成数组并返回


# 对train、val数据进行实例化
train_sents, train_labels = create_padded_dataset(train_dataset)  # 实例化训练集
val_sents, val_labels = create_padded_dataset(val_dataset)  # 实例化测试集
print("跟踪:")
print(train_labels.shape,train_labels.shape[0])
print(val_labels.shape,val_labels.shape[0])
# print(train_labels)
# train_labels = train_labels.reshape(11338, 1)  # 标签数据大小转换
# train_labels = train_labels.reshape(832475, 1)  # 标签数据大小转换
# train_labels = train_labels.reshape(train_labels.shape[0]*train_labels.shape[1], 1)  # 标签数据大小转换
train_labels = train_labels.reshape(train_labels.shape[0] , 1)  # 标签数据大小转换
# val_labels = val_labels.reshape(11338, 1)
# val_labels = val_labels.reshape(832475, 1)
# val_labels = val_labels.reshape(val_labels.shape[0]*val_labels.shape[1], 1)
val_labels = val_labels.reshape(val_labels.shape[0] , 1)
# 查看数据大小及举例内容
print("查看数据大小及举例内容:")
print(train_sents.shape)
print(train_labels.shape)
print(val_sents.shape)
print(val_labels.shape)



# 继承paddle.io.Dataset对数据进行处理
class THUCNewsDataset(paddle.io.Dataset):
    '''
    继承paddle.io.Dataset类进行封装数据
    '''

    def __init__(self, sents, labels):
        # 数据读取
        self.sents = sents
        self.labels = labels

    def __getitem__(self, index):
        # 数据处理
        data = self.sents[index]
        label = self.labels[index]

        return data, label

    def __len__(self):
        # 返回大小数据
        return len(self.sents)


# 数据实例化
train_dataset = THUCNewsDataset(train_sents, train_labels)
val_dataset = THUCNewsDataset(val_sents, val_labels)

# 封装成生成器
# DataLoader返回一个迭代器
train_loader = paddle.io.DataLoader(train_dataset, return_list=True,
                                    shuffle=True, batch_size=batch_size, drop_last=True)
val_loader = paddle.io.DataLoader(val_dataset, return_list=True,
                                  shuffle=True, batch_size=batch_size, drop_last=True)

# 查看生成器内的数据内容及大小
for i in train_loader:
    print(i)
    break
for j in val_loader:
    print(j)
    break

```


## 二、训练模型

```python
# 定义网络结构

# 网络定义
# 线性
# # 定义网络
# class MyNet(paddle.nn.Layer):
#     def __init__(self):
#         super(MyNet, self).__init__()
#         self.emb = paddle.nn.Embedding(vocab_size, emb_size)  # 嵌入层用于自动构造一个二维embedding矩阵
#         self.fc = paddle.nn.Linear(in_features=emb_size, out_features=96)  # 线性变换层
#         self.fc1 = paddle.nn.Linear(in_features=96, out_features=14)  # 分类器 class_dim
#         self.dropout = paddle.nn.Dropout(0.5)  # 正则化

#     def forward(self, x):
#         x = self.emb(x)
#         x = paddle.mean(x, axis=1)  # 获取平均值
#         x = self.dropout(x)
#         x = self.fc(x)
#         x = self.dropout(x)
#         x = self.fc1(x)
#         return x

#bilstm+att
#attention部分
class AttentionLayer(paddle.nn.Layer):
    def __init__(self, hidden_size, init_scale=0.1):
        super(AttentionLayer, self).__init__()
        self.w = paddle.create_parameter(shape=[hidden_size, hidden_size], dtype="float32")
        self.v = paddle.create_parameter(shape=[1, hidden_size], dtype="float32")

    def forward(self, inputs):
        # inputs:  [batch_size, seq_len, hidden_size]
        last_layers_hiddens = inputs
        # transposed inputs: [batch_size, hidden_size, seq_len]
        inputs = paddle.transpose(inputs, perm=[0, 2, 1])
        # inputs: [batch_size, hidden_size, seq_len]
        inputs = paddle.tanh(paddle.matmul(self.w, inputs))
        # attn_weights: [batch_size, 1, seq_len]
        attn_weights = paddle.matmul(self.v, inputs)
        # softmax数值归一化
        attn_weights = F.softmax(attn_weights, axis=-1)
        # 通过attention后的向量值, attn_vectors: [batch_size, 1, hidden_size]
        attn_vectors = paddle.matmul(attn_weights, last_layers_hiddens)
        # attn_vectors: [batch_size, hidden_size]
        attn_vectors = paddle.squeeze(attn_vectors, axis=1)

        return attn_vectors

class MyNet(paddle.nn.Layer):
    # def __init__(self, hidden_size=128, n_classes=14, n_layers=2, direction="bidirectional",dropout_rate=0.5, init_scale=0.1):
    def __init__(self, hidden_size=128, n_classes=class_dim, n_layers=2, direction="bidirectional",dropout_rate=0.5, init_scale=0.1):
        super(MyNet, self).__init__()
        # 表示LSTM单元的隐藏神经元数量，它也将用来表示hidden和cell向量状态的维度
        self.hidden_size = hidden_size
        # 表示词向量的维度
        self.embedding_size = emb_size
        # 表示神经元的dropout概率
        self.dropout_rate = dropout_rate
        # 表示词典的的单词数量
        self.vocab_size = vocab_size
        # 表示文本分类的类别数量
        self.n_classes = n_classes
        # 表示LSTM的层数
        self.n_layers = n_layers
        # 用来设置参数初始化范围
        self.init_scale = init_scale

        # 定义embedding层
        self.embedding = paddle.nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embedding_size,
                                             weight_attr=paddle.ParamAttr(
                                                 initializer=I.Uniform(low=-self.init_scale, high=self.init_scale)))
        # 定义LSTM，它将用来编码网络
        self.lstm = paddle.nn.LSTM(input_size=self.embedding_size, hidden_size=self.hidden_size,
                                   num_layers=self.n_layers, direction=direction, dropout=self.dropout_rate)


        # 对词向量进行dropout
        self.dropout_emb = paddle.nn.Dropout(p=self.dropout_rate, mode="upscale_in_train")

        # 定义Attention层
        self.attention = AttentionLayer(hidden_size=hidden_size*2 if direction == "bidirectional" else hidden_size)

        # 定义分类层，用于将语义向量映射到相应的类别
        self.cls_fc = paddle.nn.Linear(in_features=self.hidden_size*2 if direction == "bidirectional" else hidden_size,
                                       out_features=self.n_classes)

    def forward(self, inputs):
        # 获取训练的batch_size
        batch_size = inputs.shape[0]
        # 获取词向量并且进行dropout
        embedded_input = self.embedding(inputs)
        if self.dropout_rate > 0.:
            embedded_input = self.dropout_emb(embedded_input)

        # 使用LSTM进行语义编码
        last_layers_hiddens, (last_step_hiddens, last_step_cells) = self.lstm(embedded_input)

        # 进行Attention, attn_weights: [batch_size, seq_len]
        attn_vectors = self.attention(last_layers_hiddens)

        # 将其通过分类线性层，获得初步的类别数值
        logits = self.cls_fc(attn_vectors)

        return logits

# 画图
def draw_process(title,color,iters,data,label):
    plt.title(title, fontsize=24)  # 标题
    plt.xlabel("iter", fontsize=20)  # x轴
    plt.ylabel(label, fontsize=20)  # y轴
    plt.plot(iters, data,color=color,label=label)   # 画图
    plt.legend()
    plt.grid()
    plt.show()
    
# 模型训练
time_now = time.strftime("%m.%d_%H:%M", time.localtime())
print(time_now)
# paddle.save(model.state_dict(),"work/MyNet/"+str(2)+"_model.pdparams")  # 保存训练文件

val_acc = [6,4,3,5]
val_loss = [0.2,0.1,0.4,0.5]
max_acc = val_acc.index(max(val_acc, key = abs))
min_loss = val_loss.index(min(val_loss, key = abs))
print(max_acc,min_loss)
if max_acc!=min_loss:
    print('max_acc,not min_loss',max_acc)
else:
    print('abs',max_acc)


# 训练模型
def train(model):
    model.train()
    opt = paddle.optimizer.Adam(learning_rate=0.001, parameters=model.parameters())  # 优化器学习率等
    # 初始值设置
    steps = 0
    Iters, total_loss, total_acc = [], [], []
    val_acc, val_loss = [], []

    for epoch in range(epochs):  # 训练循环
        for batch_id, data in enumerate(train_loader):  # 数据循环
            steps += 1
            sent = data[0]  # 获取数据
            label = data[1]  # 获取标签

            logits = model(sent)  # 输入数据
            loss = paddle.nn.functional.cross_entropy(logits, label)  # loss获取
            acc = paddle.metric.accuracy(logits, label)  # acc获取

            if batch_id % 500 == 0:  # 每500次输出一次结果
                Iters.append(steps)  # 保存训练轮数
                total_loss.append(loss.numpy()[0])  # 保存loss
                total_acc.append(acc.numpy()[0])  # 保存acc

                print(" batch_id: {}, epoch: {},loss is: {},acc is :{} ".format(batch_id,epoch,  loss.numpy(),acc.numpy() ))  # 输出结果

            # 数据更新
            loss.backward()
            opt.step()
            opt.clear_grad()

            # 每一个epochs进行一次评估
        model.eval()
        accuracies = []
        losses = []

        for batch_id, data in enumerate(val_loader):  # 数据循环读取

            sent = data[0]  # 训练内容读取
            label = data[1]  # 标签读取

            logits = model(sent)  # 训练数据
            loss = paddle.nn.functional.cross_entropy(logits, label)  # loss获取
            acc = paddle.metric.accuracy(logits, label)  # acc获取

            accuracies.append(acc.numpy())  # 添加数据
            losses.append(loss.numpy())

        avg_acc, avg_loss = np.mean(accuracies), np.mean(losses)  # 获取loss、acc平均值
        val_acc.append(avg_acc)
        val_loss.append(avg_loss)
        print("[validation] accuracy: {}, loss: {}".format(avg_acc, avg_loss))  # 输出值

        model.train()

        # time_now = time.strftime("%m.%d_%H:%M", time.localtime())
        # paddle.save(model.state_dict(),"model/"+str(time_now)+"_model.pdparams")  # 保存训练文件
        paddle.save(model.state_dict(), os.path.join(model_save_dir,"Bilstm_att10/" + str(epoch) + "_model.pdparams"))  # 保存训练文件

    # 迭代完,查看val_acc和val_loss的情况：
    max_acc = val_acc.index(max(val_acc, key=abs))
    min_loss = val_loss.index(min(val_loss, key=abs))
    print(max_acc, min_loss)
    if max_acc != min_loss:
        print('max_acc,not min_loss', max_acc)
    else:
        print('abs', max_acc)
    print('val_acc:', val_acc)
    print('val_loss:', val_loss)

    draw_process("trainning loss", "red", Iters, total_loss, "trainning loss")  # 画处loss图
    draw_process("trainning acc", "green", Iters, total_acc, "trainning acc")  # 画出caa图

# if False:
if True:
    model = MyNet()  # 模型实例化
    print("开始训练")
    train(model)  # 开始训练

```

## 三、使用模型进行预测

```python

# 预测
# -*- coding:utf-8 -*-
import sys

import paddle
import numpy as np
import matplotlib.pyplot as plt
import paddle.nn as nn
import os
import pandas as pd
import numpy as np
import time
import paddle.nn.functional as F
import paddle.nn.initializer as I

'''
使用paddlepaddle 2.0.2
'''
# 定义公共变量
data_root_path = 'work/goods/'  # 数据集根目录
# data_file = 'news_classify_data.txt'  # 数据集名称
data_file = 'data.txt'  # 数据集名称
test_file = 'Test_IDs.txt'  # 测试集
valid_file = 'Val_IDs.txt'  # 测试集
train_file = 'Train_IDs.txt'  # 训练集
dict_file = 'dict.txt'  # 编码生成的字典
model_save_dir = 'model/goods/'

label_list=[]
labels_fileName= os.path.join(data_root_path,"labels.txt") 
with open(labels_fileName,encoding='utf-8') as file:
    content=file.read()
    # print(content)
    label_list=content.split("\n")
# label_list=['蔬菜', '家电', '乳品', '酒饮冲调', '家居', '个护', '休闲食品', '清洁', '水产', '水果', '快手美食', '粮油速食', '肉禽蛋', '3C数码', '健康', '宠物', '美妆', '母婴', '玩具', '鲜花绿植', '餐饮', '钟表配饰', '健康食品', '汽车用品', '内衣', '童装', '箱包', '办公', '运动', '书籍', '礼品', '香烟', '男装', '女装', '鞋']
print("labels:",label_list)


# 分类器
class_dim=len(label_list)

# 字典读取
def get_dict_len(d_path):
    with open(d_path, 'r', encoding='utf-8') as f:
        line = eval(f.readlines()[0])
    return line

word_dict = get_dict_len( os.path.join(data_root_path,  'dict.txt'))

# 数据初始化
# 初始数据准备
vocab_size = len(word_dict) + 1  # 字典长度加1
emb_size = 256  # 神经网络长度
seq_len = 30  # 数据集长度（需要扩充的长度）
batch_size = 32  # 批处理大小
epochs = 50  # 训练轮数
pad_id = word_dict['<unk>']  # 空的填充内容值


# 预测
print("开始预测")

#bilstm+att
#attention部分
class AttentionLayer(paddle.nn.Layer):
    def __init__(self, hidden_size, init_scale=0.1):
        super(AttentionLayer, self).__init__()
        self.w = paddle.create_parameter(shape=[hidden_size, hidden_size], dtype="float32")
        self.v = paddle.create_parameter(shape=[1, hidden_size], dtype="float32")

    def forward(self, inputs):
        # inputs:  [batch_size, seq_len, hidden_size]
        last_layers_hiddens = inputs
        # transposed inputs: [batch_size, hidden_size, seq_len]
        inputs = paddle.transpose(inputs, perm=[0, 2, 1])
        # inputs: [batch_size, hidden_size, seq_len]
        inputs = paddle.tanh(paddle.matmul(self.w, inputs))
        # attn_weights: [batch_size, 1, seq_len]
        attn_weights = paddle.matmul(self.v, inputs)
        # softmax数值归一化
        attn_weights = F.softmax(attn_weights, axis=-1)
        # 通过attention后的向量值, attn_vectors: [batch_size, 1, hidden_size]
        attn_vectors = paddle.matmul(attn_weights, last_layers_hiddens)
        # attn_vectors: [batch_size, hidden_size]
        attn_vectors = paddle.squeeze(attn_vectors, axis=1)

        return attn_vectors

class MyNet(paddle.nn.Layer):
    # def __init__(self, hidden_size=128, n_classes=14, n_layers=2, direction="bidirectional",dropout_rate=0.5, init_scale=0.1):
    def __init__(self, hidden_size=128, n_classes=class_dim, n_layers=2, direction="bidirectional",dropout_rate=0.5, init_scale=0.1):
        super(MyNet, self).__init__()
        # 表示LSTM单元的隐藏神经元数量，它也将用来表示hidden和cell向量状态的维度
        self.hidden_size = hidden_size
        # 表示词向量的维度
        self.embedding_size = emb_size
        # 表示神经元的dropout概率
        self.dropout_rate = dropout_rate
        # 表示词典的的单词数量
        self.vocab_size = vocab_size
        # 表示文本分类的类别数量
        self.n_classes = n_classes
        # 表示LSTM的层数
        self.n_layers = n_layers
        # 用来设置参数初始化范围
        self.init_scale = init_scale

        # 定义embedding层
        self.embedding = paddle.nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embedding_size,
                                             weight_attr=paddle.ParamAttr(
                                                 initializer=I.Uniform(low=-self.init_scale, high=self.init_scale)))
        # 定义LSTM，它将用来编码网络
        self.lstm = paddle.nn.LSTM(input_size=self.embedding_size, hidden_size=self.hidden_size,
                                   num_layers=self.n_layers, direction=direction, dropout=self.dropout_rate)


        # 对词向量进行dropout
        self.dropout_emb = paddle.nn.Dropout(p=self.dropout_rate, mode="upscale_in_train")

        # 定义Attention层
        self.attention = AttentionLayer(hidden_size=hidden_size*2 if direction == "bidirectional" else hidden_size)

        # 定义分类层，用于将语义向量映射到相应的类别
        self.cls_fc = paddle.nn.Linear(in_features=self.hidden_size*2 if direction == "bidirectional" else hidden_size,
                                       out_features=self.n_classes)

    def forward(self, inputs):
        # 获取训练的batch_size
        batch_size = inputs.shape[0]
        # 获取词向量并且进行dropout
        embedded_input = self.embedding(inputs)
        if self.dropout_rate > 0.:
            embedded_input = self.dropout_emb(embedded_input)

        # 使用LSTM进行语义编码
        last_layers_hiddens, (last_step_hiddens, last_step_cells) = self.lstm(embedded_input)

        # 进行Attention, attn_weights: [batch_size, seq_len]
        attn_vectors = self.attention(last_layers_hiddens)

        # 将其通过分类线性层，获得初步的类别数值
        logits = self.cls_fc(attn_vectors)

        return logits


# 预测数据读取

# 比赛数据读取
set = []

def dataset(datapath):
    with open(datapath) as f:  # 读取文件
        for i in f.readlines():  # 逐行读取数据
            dataset = np.array(i.split(','))  # 分割数据
            set.append(dataset)  # 存入数据
    return set


# 比赛数据扩充
def create_padded_dataset(dataset):
    padded_sents = []
    labels = []
    for batch_id, data in enumerate(dataset):  # 循环
        # print(data)
        sent = data  # 读取数据
        padded_sent = np.concatenate([sent[:seq_len], [pad_id] * (seq_len - len(sent))]).astype('int32')  # 拼接填充

        # print(padded_sent)
        padded_sents.append(padded_sent)  # 输入数据

    # print(padded_sents)
    return np.array(padded_sents)  # 转换成数组并返回


test_data = dataset(os.path.join(data_root_path,'Test_IDs.txt'))  # 读取数据
# print()
# 对train、val数据进行实例化
test_data = create_padded_dataset(test_data)  # 数据填充

# 查看数据大小及举例内容
print(test_data)


# 导入模型
# model_state_dict = paddle.load('work/Bilstm_att10/5_model.pdparams')  # 模型读取
model_state_dict = paddle.load(os.path.join(model_save_dir,'Bilstm_att10/5_model.pdparams'))  # 模型读取
model = MyNet()  # 读取网络
model.set_state_dict(model_state_dict)
model.eval()
# print(type(test_data[0]))
count = 0  # 初始值
# with open('./result_bilstm_att.txt', 'w', encoding='utf-8') as f_train:  # 生成文件


test_file_path=os.path.join(data_root_path, "Test.txt")
with open(test_file_path, encoding='utf-8') as f:
    lines = f.readlines()

# 生成预测结果
with open(os.path.join(data_root_path,'Test_result_bilstm_att.txt'), 'w', encoding='utf-8') as f_train:  # 生成文件
    for batch_id, data in enumerate(test_data):  # 循环数据
        # print(data)
        results = model(paddle.to_tensor(data.reshape(30, 1)))  # 开始训练

        for probs in results:
            # 映射分类label
            print(probs)
            idx = np.argmax(probs)  # 获取结果值
            labels = label_list[idx]  # 通过结果值获取标签
            print("{}%".format((probs.numpy()[idx])*100))
            # f_train.write( lines[count].split('\t')[-1].replace('\n', '')+"\t"+ labels + "\n")  # 写入数据
            f_train.write( lines[count].replace('\n', '')+"\t"+ labels +"\t"+"{}%".format((probs.numpy()[idx]))+ "\n")  # 写入数据
            count += 1
            break

        if count % 500 == 0:  # 查看推理情况
            print(count)

print(count)


```
