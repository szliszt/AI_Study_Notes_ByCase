# 相关算法

## 1 激活算法
### 1.1 softmax函数
softmax函数，又称归一化指数函数。它是二分类函数sigmoid在多分类上的推广，目的是将多分类的结果以概率的形式展现出来。下图展示了softmax的计算方法：


$$ softmax(z_i)={e^{z_i}\over{ \sum\limits_{c=1}^C e^{z_c} }} $$

![image](https://note.youdao.com/yws/res/45918/37031AC00C5A4FB9B4AC3FC83D1A6A97)

不使用框架，用原生python代码：


```python
import math

z = [1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0]

z_exp = [math.exp(i) for i in z]

print(z_exp)  # Result: [2.72, 7.39, 20.09, 54.6, 2.72, 7.39, 20.09]

sum_z_exp = sum(z_exp)
print(sum_z_exp)  # Result: 114.98
# Result: [0.024, 0.064, 0.175, 0.475, 0.024, 0.064, 0.175]

softmax = [round(i / sum_z_exp, 3) for i in z_exp]
print(softmax)

```



tensorflow代码：

```python
import tensorflow as tf

print(tf.__version__) # 2.0.0
a = tf.constant([2, 3, 5], dtype = tf.float32)

b1 = a / tf.reduce_sum(a) # 不使用指数
print(b1) # tf.Tensor([0.2 0.3 0.5], shape=(3,), dtype=float32)

b2 = tf.nn.softmax(a) # 使用指数的Softmax
print(b2) # tf.Tensor([0.04201007 0.11419519 0.8437947 ], shape=(3,), dtype=float32)

```

paddlepaddle代码:


```python
import paddle
import numpy as np

x = np.array([[[-2.0, 3.0, -4.0, 5.0],
                [3.0, -4.0, 5.0, -6.0],
                [-7.0, -8.0, 8.0, 9.0]],
                [[1.0, -2.0, -3.0, 4.0],
                [-5.0, 6.0, 7.0, -8.0],
                [6.0, 7.0, 8.0, 9.0]]], 'float32')
x = paddle.to_tensor(x)
m = paddle.nn.Softmax()
out = m(x)
print(out)
# [[[0.0320586 , 0.08714432, 0.23688282, 0.64391426],
#   [0.0320586 , 0.08714432, 0.23688282, 0.64391426],
#   [0.07232949, 0.19661193, 0.19661193, 0.53444665]],
# [[0.0320586 , 0.08714432, 0.23688282, 0.64391426],
#   [0.0320586 , 0.08714432, 0.23688282, 0.64391426],
#   [0.0320586 , 0.08714432, 0.23688282, 0.64391426]]]

```

pytorch代码:


```python
import torch
import torch.nn as nn

input_0 = torch.Tensor([1,2,3,4])
input_1 = torch.Tensor([[1,2,3,4],[5,6,7,8]])
#规定不同方向的softmax
softmax_0 = nn.Softmax(dim=0)
softmax_1 = nn.Softmax(dim=1 )
#对不同维度的张量试验
output_0 = softmax_0(input_0)
output_1 = softmax_1(input_1)
output_2 = softmax_0(input_1)
#输出
print("输出:")
print(output_0)
print(output_1)
print(output_2)

```


### 1.2 sigmoid
Sigmoid函数是一个生物学中常见的S型函数，也称为S型生长曲线。在信息科学中，由于其单增以及反函数单增等性质，Sigmoid函数常被用作神经网络的激活函数，将变量映射到0,1之间。


$$
S(x)={1\over 1+e^{-x}}
$$

![image](https://note.youdao.com/yws/res/45961/68B43CAC3A0A4B05A9FA857E99230CAC)

代码:


```python
import matplotlib.pyplot as plt
import numpy as np
import math

x = np.linspace(-10, 10, 100)
z = 1 / (1 + np.exp(-x))

plt.title("Sigmoid")
plt.plot(x, z)
plt.xlabel("x")
plt.ylabel("Sigmoid(X)")

plt.savefig("sigmoid.png")
plt.show()
plt.close()


```

pytorch代码

```python
import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt

x=torch.linspace(-5,5,200) #制造数据，-5~5之间等距离取200个点作为x坐标值
x_np=x.numpy() #tensor转成numpy形式
y_sigmoid=torch.sigmoid(x).numpy() #用sigmoid函数求值

plt.plot(x_np,y_sigmoid,c='red',label='sigmoid') #画图
plt.ylim(-0.2,1.2)
plt.legend(loc='best')
plt.show()

```


paddle代码

```python
import paddle

m = paddle.nn.Sigmoid()
x = paddle.to_tensor([1.0, 2.0, 3.0, 4.0])
out = m(x) # [0.7310586, 0.880797, 0.95257413, 0.98201376]
print(out)

```


### 1.3 ReLU函数
线性整流函数（Linear rectification function），又称修正线性单元，是一种人工神经网络中常用的激活函数（activation function），通常指代以斜坡函数及其变种为代表的非线性函数。

f(x)=max(0,x)

![image](https://note.youdao.com/yws/res/45959/687733DAF37C4085B1EAE821558C349B)


python代码：

```python
# plot inputs and outputs
from matplotlib import pyplot

# rectified linear function
def rectified(x):
  return max(0.0, x)

# define a series of inputs
series_in = [x for x in range(-10, 11)]
# calculate outputs for our inputs
series_out = [rectified(x) for x in series_in]
# line plot of raw inputs to rectified outputs
pyplot.plot(series_in, series_out)
pyplot.show()


```

paddle框架

```python
import paddle
import paddle.nn.functional as F
import numpy as np

x = paddle.to_tensor(np.array([-2, 0, 1]).astype('float32'))
out = F.relu(x) 
print(out)
# [0., 0., 1.]

```



### 1.4 Tanh函数
Tanh函数是在其函数表达式的基础上经过放大或缩小处理后的结果，Tanh函数在一定程度上改善了sidmoid函数,但是梯度消失的问题仍然没有完全避免


$$
tanh(x)={\frac{e^x-e^{-x}}{e^x+e^{-x}} }
$$

![image](https://note.youdao.com/yws/res/45972/2F541077911247CEB7C2D66D1ECCB559)

python代码


```python
# 导入模块
import numpy as np
import matplotlib.pyplot as plt

# 生成数据
x = np.linspace(-5, 5, 500)  # numpy.linspace()函数用于在线性空间中以均匀步长生成数字序列。

# 定义函数
def sigm(x):
    return 1 / (1 + np.exp(-x))

sigmoid = sigm(x)
tanh = np.tanh(x)
relu = np.maximum(x, 0)
# ReLU激活函数的提出就是为了解决梯度消失问题。
## ReLU的梯度只可以取两个值：0或1，当输入小于0时，梯度为0；当输入大于0时，梯度为x。

# draw all funcations
plt.plot(x, sigmoid, label='sigmoid')
plt.plot(x, tanh, label='tanh')
plt.plot(x, relu, label='relu')
plt.legend(loc='best')
plt.xlabel('x')
plt.ylabel('y')
plt.xlim(-5, 5)
plt.ylim(-1, 2)
plt.show()


```

