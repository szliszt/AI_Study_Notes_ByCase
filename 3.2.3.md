# 基于飞桨实现五位验证码识别

```python
import os 
os.environ["CUDA_VISIBLE_DEVICES"] = "0"
```

## 模型介绍
本项目中使用的模型为CNN，先使用卷积层提取图像特征，然后归一化处理，再输入到激活函数层，激活函数为Relu；然后进行池化操作，总共七层； 然后一个GRU，加强神经网络的记忆能力；最后是输出层，与对应类型数匹配（63 = 26个小写字母 + 26个大写字母 + 10个数字 + 1个空格）

**Conv2D->MaxPooling2D->Relu -> Conv2D->Maxpooling2D->Relu -> Conv2D->MaxPooling2D->Relu -> Conv2D->Maxpooling2D->Relu -> Conv2D->MaxPooling2D->Relu -> Conv2D->Maxpooling2D->Relu -> Conv2D->Maxpooling2D->Relu -> Linear -> GRU -> Linear
**

```python
# 神经网络模型
import paddle
import paddle.nn as nn


class Model(nn.Layer):
    def __init__(self, vocabulary):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2D(in_channels=1, out_channels=32, kernel_size=4)
        self.relu = nn.ReLU()
        self.bn1 = nn.BatchNorm2D(32)
        self.pool1 = nn.MaxPool2D(kernel_size=2, stride=1)

        self.conv2 = nn.Conv2D(in_channels=32, out_channels=64, kernel_size=4)
        self.bn2 = nn.BatchNorm2D(64)
        self.pool2 = nn.MaxPool2D(kernel_size=2, stride=1)

        self.conv3 = nn.Conv2D(in_channels=64, out_channels=128, kernel_size=4)
        self.bn3 = nn.BatchNorm2D(128)
        self.pool3 = nn.MaxPool2D(kernel_size=2, stride=1)

        self.conv4 = nn.Conv2D(in_channels=128, out_channels=256, kernel_size=4)
        self.bn4 = nn.BatchNorm2D(256)
        self.pool4 = nn.MaxPool2D(kernel_size=2, stride=1)

        self.conv5 = nn.Conv2D(in_channels=256, out_channels=256, kernel_size=4)
        self.bn5 = nn.BatchNorm2D(256)
        self.pool5 = nn.MaxPool2D(kernel_size=2, stride=1)

        self.conv6 = nn.Conv2D(in_channels=256, out_channels=256, kernel_size=4)
        self.bn6 = nn.BatchNorm2D(256)
        self.pool6 = nn.MaxPool2D(kernel_size=2, stride=1)

        self.conv7 = nn.Conv2D(in_channels=256, out_channels=256, kernel_size=4)
        self.bn7 = nn.BatchNorm2D(256)
        self.pool7 = nn.MaxPool2D(kernel_size=2, stride=1)

        self.fc = nn.Linear(in_features=244, out_features=128)

        self.gru = nn.GRU(input_size=256, hidden_size=128)

        self.output = nn.Linear(in_features=128, out_features=len(vocabulary) + 1)

    def forward(self, x):
        x = self.relu(self.bn1(self.conv1(x)))
        x = self.pool1(x)
        x = self.relu(self.bn2(self.conv2(x)))
        x = self.pool2(x)
        x = self.relu(self.bn3(self.conv3(x)))
        x = self.pool3(x)
        x = self.relu(self.bn4(self.conv4(x)))
        x = self.pool4(x)
        x = self.relu(self.bn5(self.conv5(x)))
        x = self.pool5(x)
        x = self.relu(self.bn6(self.conv6(x)))
        x = self.pool6(x)
        x = self.relu(self.bn7(self.conv7(x)))
        x = self.pool7(x)
        x = paddle.reshape(x, shape=(x.shape[0], x.shape[1], -1))
        x = self.fc(x)
        x = paddle.transpose(x, perm=[2, 0, 1])
        y, h = self.gru(x)
        x = self.output(y)
        return x
        
```

## 数据处理过程

```python

# 数据预处理

# 解压两个压缩包
import zipfile
import os 

def make_dir(a_file):
    if os.path.exists(a_file):
        return
    else:
        os.makedirs(a_file)

make_dir('data/train_jpg')
make_dir('data/test_jpg')
def unzip(zpath, upath):
    frzip = zipfile.ZipFile(zpath, 'r', zipfile.ZIP_DEFLATED)
    names = frzip.namelist()
    for name in names:
        frzip.extract(name, upath)

# 从挂载的数据集解压缩到当前目录下
unzip('/home/aistudio/data/data172520/19999train.zip', 'data/train_jpg')
unzip('/home/aistudio/data/data172520/8000test.zip', 'data/test_jpg')

# 将训练集和测试集分别制表
import os

def make_list(path_0, path_1):
    imgs = os.listdir(path_0)
    for img in imgs:
        if(img == '.ipynb_checkpoints'):
            imgs.remove(img)
    imgs.sort()
    with open(path_1, 'w', encoding='utf-8') as f:
        for img in imgs:
            file_name = img.split('.')[0]
            file_path = os.path.join(path_0, img)
            f.write(file_path + '\t' + file_name + '\n')

make_list('data/train_jpg/', 'data/train.txt')
make_list('data/test_jpg/', 'data/test.txt')

with open('data/train.txt', 'r', encoding='utf-8') as f:
    lines = f.readlines()
v = set()
for line in lines:
    _, label = line.replace('\n', '').split('\t')
    for c in label:
        v.add(c)

vocabulary_path = 'data/vocabulary.txt'
with open(vocabulary_path, 'w', encoding='utf-8') as f:
    for c in v:
        f.write(c + '\n')
        
        
```

```python

#图像预处理和数据加载器
import cv2
import numpy as np
from paddle.io import Dataset


# 图像预处理
def process(path):
    image = cv2.imread(path)
    # 转灰度图
    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    # 转换成CHW
    image = image[np.newaxis, :]
    # 归一化
    image = (image - 128) / 128
    return image


# 数据加载器
class CustomDataset(Dataset):
    def __init__(self, data_list_path, voc_path):
        super(CustomDataset, self).__init__()
        with open(data_list_path, 'r', encoding='utf-8') as f:
            self.lines = f.readlines()
        with open(voc_path, 'r', encoding='utf-8') as f:
            labels = f.readlines()
        self.vocabulary = [labels[i].replace('\n', '') for i in range(len(labels))]
        self.vocabulary_dict = dict([(labels[i].replace('\n', ''), i) for i in range(len(labels))])

    def __getitem__(self, idx):
        path, label = self.lines[idx].replace('\n', '').split('\t')
        img = process(path)
        # 将字符标签转换为int数据
        transcript = [self.vocabulary_dict.get(x) for x in label]
        img = np.array(img, dtype='float32')
        transcript = np.array(transcript, dtype='int32')
        return img, transcript

    def __len__(self):
        return len(self.lines)
        
```


```python

#解码器
from itertools import groupby
import paddle


def ctc_greedy_decoder(probs_seq, vocabulary):
    # argmax以获得每个时间步长的最佳指标
    max_index_list = paddle.argmax(probs_seq, -1).numpy()
    # 删除连续的重复索引
    index_list = [index_group[0] for index_group in groupby(max_index_list)]
    # 删除空白索引
    blank_index = len(vocabulary)
    index_list = [index for index in index_list if index != blank_index]
    # 将索引列表转换为字符串
    return ''.join([vocabulary[index] for index in index_list])[:5]


def label_to_string(label, vocabulary):
    #标签转文字
    return ''.join([vocabulary[index] for index in label])
    
    
```


## 模型训练过程
总共训练50轮，每个批次大小为16，学习率设置为0.001
- <font color=red>此代码运行速度比较慢！</font>

- <font color=red>   强烈建议将本代码下载到本地执行！</font>

```python

#开始训练
import paddle
import numpy as np
import os
from datetime import datetime
from paddle.io import DataLoader
from paddle.static import InputSpec

train_data_list_path = 'data/train.txt'
test_data_list_path = 'data/test.txt'
voc_path = 'data/vocabulary.txt'
save_model = 'models/model1'  # 模型保存的位置
batch_size = 16  # 每批喂16组数据
num_epoch = 50  # 训练50轮
pretrained_model = None
learning_rate = 1e-3  # 学习率 0.001


def train():
    # 获取训练数据
    train_dataset = CustomDataset(train_data_list_path, voc_path)
    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)
    # 获取测试数据
    test_dataset = CustomDataset(test_data_list_path, voc_path)
    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size)
    # 获取模型
    model = Model(train_dataset.vocabulary)
    paddle.summary(model, input_size=(batch_size, 1, 30, 150))
    # 设置优化方法
    boundaries = [10, 30]
    lr = [0.1 ** l * learning_rate for l in range(len(boundaries) + 1)]
    scheduler = paddle.optimizer.lr.PiecewiseDecay(boundaries=boundaries, values=lr)
    optimizer = paddle.optimizer.Adam(parameters=model.parameters(), learning_rate=scheduler)
    # 获取损失函数
    ctc_loss = paddle.nn.CTCLoss(blank=len(train_dataset.vocabulary))
    train_step = 0
    # 开始训练
    for epoch in range(num_epoch):
        for batch_id, (inputs, labels) in enumerate(train_loader()):
            out = model(inputs)
            input_lengths = paddle.full(shape=[out.shape[1]], fill_value=out.shape[0], dtype="int64")
            label_lengths = paddle.full(shape=[out.shape[1]], fill_value=5, dtype="int64")
            # 计算损失
            loss = ctc_loss(out, labels, input_lengths, label_lengths)
            loss.backward()
            optimizer.step()
            optimizer.clear_grad()
            if batch_id % 400 == 0:
                print('[%s] Train epoch %d, batch %d, loss: %f' % (datetime.now(), epoch, batch_id, loss))
                train_step += 1
        # 保存模型
        paddle.jit.save(layer=model, path=save_model, input_spec=[InputSpec(shape=[None, 1, 30, 150], dtype='float32')])


train()

```


## 提交结果

```python

#使用训练出的模型预测结果
import csv
import numpy as np
import paddle

data_path = 'data/test.txt'
result_path = 'models/result.csv'

with open('data/vocabulary.txt', 'r', encoding='utf-8') as f:
    vocabulary = f.readlines()

vocabulary = [v.replace('\n', '') for v in vocabulary]

save_model = 'models/model1'
model = paddle.jit.load(save_model)
model.eval()

def infer(path, label):
    data = process(path)
    data = data[np.newaxis, :]
    data = paddle.to_tensor(data, dtype='float32')
    # 执行识别
    out = model(data)
    out = paddle.transpose(out, perm=[1, 0, 2])
    out = paddle.nn.functional.softmax(out)[0]
    # 解码获取识别结果
    out_string = ctc_greedy_decoder(out, vocabulary)

    print(label, '：%s' % out_string)
    return out_string

with open(data_path, 'r', encoding='utf-8') as f:
    lines = f.readlines()


def get_path(index):
    path, label = lines[index].replace('\n', '').split('\t')
    return path, label

with open(result_path, 'w', newline='', encoding='utf-8') as csvfile:
    writer = csv.writer(csvfile)
    writer.writerow(["QID", "Prediction"])
    for i in range(0, 8000):
        image_path, idx = get_path(i)
        result_str = infer(image_path, idx)
        writer.writerow(['%s' % i, '%s' % result_str + "Q", '\n'])
        
        
```








