
```

```
# 机器学习
## 1、 使用线性规划预测考试分数


> 假设学习时长与考试分数有线性关系


```python

import numpy as np
from sklearn import linear_model

# 训练数据，每一行表示一个样本，包含的信息分别为：
# 学习时间与学习成绩的关系
# 学习时长
x = np.array([ 23.80 , 27.60 , 31.60 , 32.40 , 33.70 , 34.90 , 43.20 , 52.80 , 63.80 , 73.40 ])
# 输入的数据只有1个特征，需要用array.reshape(-1, 1)来改变数组的形状,转变成多维数组
x=x.reshape(-1,1)

# 考试得分
y = np.array([41.4,51.8,61.70,67.90,68.70,77.50,95.90,137.40,155.0,175.0])
y=y.reshape(-1,1)

# 创建线性回归模型
lr = linear_model.LinearRegression()
# 根据已知数据拟合最佳直线，即训练模型
lr.fit(x, y)

# 待测的未知数据，学习时长是50
data_test= [[50]]
# 预测考试分数
score=lr.predict(data_test)
print("考试得分:",score[0][0])

# 绘图
import matplotlib.pyplot as plt
# 拉直成一维数组
x_train=x.reshape(1,-1)
y_train=y.reshape(1,-1)
# 画散点图
plt.scatter(x_train,y_train,color='blue',label='train data')
# 根据学习时长推算出的成绩
y_train_pred = lr.predict(x)
# 把推测结果画出一条斜线
plt.plot(x_train[0],y_train_pred.reshape(1,-1)[0],color='red',label='best line')
# plt.scatter(x_train,y_train_pred.reshape(1,-1),color='black',label='test data')
plt.legend(loc=2)
plt.xlabel('Hours')
plt.ylabel('Score')
# 本机显示图像
plt.show()
# jpuyter显示图像
plt

```



参考: https://zhuanlan.zhihu.com/p/133379871?ivk_sa=1024320u

## 2、 使用线性回归预测儿童身高

> 理论上，一个人的身高除了随年龄变大而增长之外，在一定程度上还受到遗传和饮食习惯以及其他因素的影响。在这里我们把问题简化一下，假定一个人的身高只受年龄、性别、父母身高、祖父母身高和外祖父母身高这几个因素的影响，并假定大致符合线性关系。
也就是说，在其他条件不变的情况下，随着年龄的增长，会越来越高；同样，对于其他条件都相同的儿童，其父母身高较大的话，儿童也会略高一些。但在实际应用时要考虑到一个情况，人的身高不是一直在变高的，到了一定年龄之后就不再生长了，然后身高会长期保持固定而不再变化（不考虑年龄太大之后会稍微变矮一点的情况）。为了简化问题，我们假设18岁之后身高不再变化。


```python

import copy
import numpy as np
from sklearn import linear_model

# 训练数据，每一行表示一个样本，包含的信息分别为：
# 儿童年龄,性别（0女1男）
# 父亲、母亲、祖父、祖母、外祖父、外祖母的身高
x = np.array([[1, 0, 180, 165, 175, 165, 170, 165],
              [3, 0, 180, 165, 175, 165, 173, 165],
              [4, 0, 180, 165, 175, 165, 170, 165],
              [6, 0, 180, 165, 175, 165, 170, 165],
              [8, 1, 180, 165, 175, 167, 170, 165],
              [10, 0, 180, 166, 175, 165, 170, 165],
              [11, 0, 180, 165, 175, 165, 170, 165],
              [12, 0, 180, 165, 175, 165, 170, 165],
              [13, 1, 180, 165, 175, 165, 170, 165],
              [14, 0, 180, 165, 175, 165, 170, 165],
              [17, 0, 170, 165, 175, 165, 170, 165]])

# 儿童身高，单位：cm
y = np.array([60, 90, 100, 110, 130, 140, 150, 164, 160, 163, 168])

# 创建线性回归模型
lr = linear_model.LinearRegression()
# 根据已知数据拟合最佳直线
lr.fit(x, y)

# 待测的未知数据，其中每个分量的含义和训练数据相同
xs = np.array([[10, 0, 180, 165, 175, 165, 170, 165],
               [17, 1, 173, 153, 175, 161, 170, 161],
               [34, 0, 170, 165, 170, 165, 170, 165]])

for item in xs:
    # 为不改变原始数据，进行深复制，并假设超过18岁以后就不再长高了
    # 对于18岁以后的年龄，返回18岁时的身高
    item1 = copy.deepcopy(item)
    if item1[0] > 18:
        item1[0] = 18
    print(item1)
    print(item1.reshape(1,-1))
    print(item, ':', lr.predict(item1.reshape(1,-1)))
    print("*"*50)


```

## 3、 用逻辑回归给虫子分类
> 有红绿两种虫子，有2个特征x1，x2 （比如颜色、尺寸、形状...），请画一条直线将它们分开



```python
import matplotlib
import matplotlib.pyplot as plt
import csv
import numpy as np
import math
import requests
import pandas as pd

# 逻辑回归
# 有一系列散点，画一条斜线将两种点划分开

def loadDataset():
    data=[]
    labels=[]

    df=  pd.read_csv('http://www.maker-game.com/download/file/logisticDataset.csv')
    for index,row in df.iterrows():
        data.append([1.0, float(row[0]), float(row[1])])
        labels.append(int(row[2]))

    return data,labels

def plotBestFit(W):
    # 把训练集数据用坐标的形式画出来
    dataMat,labelMat=loadDataset()
    dataArr = np.array(dataMat)
    n = np.shape(dataArr)[0]
    xcord1 = []
    ycord1 = []
    xcord2 = []
    ycord2 = []
    for i in range(n):
        if int(labelMat[i])== 1:
            xcord1.append(dataArr[i,1]); ycord1.append(dataArr[i,2])
        else:
            xcord2.append(dataArr[i,1]); ycord2.append(dataArr[i,2])
    fig = plt.figure()
    ax = fig.add_subplot(111)
    ax.scatter(xcord1, ycord1, s=30, c='red', marker='s')
    ax.scatter(xcord2, ycord2, s=30, c='green')

    # 把分类边界画出来
    x = np.arange(-3.0,3.0,0.1)
    y = (-W[0]-W[1]*x)/W[2]
    ax.plot(x,y)
    plt.show()
    plt

def plotloss(loss_list):
    x = np.arange(0,30,0.01)
    plt.plot(x,np.array(loss_list),label = 'linear')

    plt.xlabel('time')       # 梯度下降的次数
    plt.ylabel('loss')       # 损失值
    plt.title('loss trend')         # 损失值随着W不断更新，不断变化的趋势
    plt.legend()               # 图形图例
    plt.show()
    plt

def main():
    # 读取训练集(txt文件)中的数据,
    data, labels = loadDataset()
    # 将数据转换成矩阵的形式，便于后面进行计算
    # 构建特征矩阵X
    X = np.array(data)
    # 构建标签矩阵y
    y = np.array(labels).reshape(-1,1)
    # 随机生成一个w参数(权重)矩阵    .reshape((-1,1))的作用是，不知道有多少行，只想变成一列
    W = 0.001*np.random.randn(3,1).reshape((-1,1))
    # m表示一共有多少组训练数据
    m = len(X)
    # 定义梯度下降的学习率 0.03
    learn_rate = 0.03

    loss_list = []
    # 实现梯度下降算法，不断更新W,获得最优解，使损失函数的损失值最小
    for i in range(3000):
        # 最重要的就是这里用numpy 矩阵计算，完成假设函数计算，损失函数计算，梯度下降计算
        # 计算假设函数 h(w)x
        g_x = np.dot(X,W)
        h_x = 1/(1+np.exp(-g_x))

        # 计算损失函数 Cost Function 的损失值loss
        loss = np.log(h_x)*y+(1-y)*np.log(1-h_x)
        loss = -np.sum(loss)/m
        loss_list.append(loss)

        # 梯度下降函数更新W权重
        dW = X.T.dot(h_x-y)/m
        W += -learn_rate*dW

    # 得到更新后的W，可视化
    print('W最优解:')
    print(W)
    print('最终得到的分类边界:')
    plotBestFit(W)
    print('损失值随着W不断更新，不断变化的趋势:')
    plotloss(loss_list)

    # 定义一个测试数据，计算他属于那一类别
    # test_x = np.array([1,-1.395634,4.662541])
    test_x = np.array([1,1.1,3.3])
    test_y = 1/(1+np.exp(-np.dot(test_x,W)))
    print("测试结果:")
    print(test_y)

#     print(data_arr)
if __name__=='__main__':
    main()


```


通过第二张图可以看到：损失值随着W不断更新，不断变化的趋势


测试结果:
[0.96338216]

**paddlepaddle版本**


```python

#引入必要的包
import paddle
print("本教程使用的paddle版本为：" + paddle.__version__)
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(0)
num=100

#生成数据集x1,x2,y0/1
#随机生成100个x1
x1=np.random.normal(6,1,size=(num))
#随机生成100个x2
x2=np.random.normal(3,1,size=(num))
#生成100个y（全身不都是1哦）
y=np.ones(num)
#将生成好的点放入到一个分类中
class1=np.array([x1,x2,y])
class1.shape
#接下来生成第二类点，原理跟第一类一样
x1=np.random.normal(3,1,size=(num))
x2=np.random.normal(6,1,size=(num))
y=np.ones(num)*(-1)
class2=np.array([x1,x2,y])

#看一下生成点的样子
print(class1.shape)
print(class2.shape)

class1=class1.T
class2=class2.T
#再看一下生成点的样子
print(class1.shape)
print(class2.shape)

# 画图
plt.scatter(class1[:,0],class1[:,1])
plt.scatter(class2[:,0],class2[:,1],marker='*')
plt

#将两类数据都放到一个变量里面
all_data = np.concatenate((class1,class2))
#将数据打乱
np.random.shuffle(all_data)
print(all_data[:10])


print(all_data.shape)
#截取出坐标数据
train_data_x=all_data[:150,:2]
#截取出标签数据
train_data_y=all_data[:150,-1].reshape(150,1)

print(train_data_x.shape)

print(train_data_y.shape)

# 现在Y不是Y轴而是一个标签。x不是单一的x输入量而是一个坐标
#将数据转化为tensor形式
x_data = paddle.to_tensor(train_data_x.astype('float32'))
y_data = paddle.to_tensor(train_data_y.astype('float32'))

# 这是要学的公式
# y=w1*x1+w2*x2+b

#初始化一个感知“鸡”
#这个感知“鸡”的公式可以去看一下官方文档哦

linear = paddle.nn.Linear(in_features=2, out_features=1)
#初始化一个优化函数帮助我们训练感知“鸡”
mse_loss = paddle.nn.MSELoss()
sgd_optimizer = paddle.optimizer.SGD(learning_rate=0.001, parameters = linear.parameters())

# 开始训练
# 定义一下要训练多少回合，这只鸡看起来就不太聪明咱们多训练一会
total_epoch = 50000
#构建训练过程
for i in range(total_epoch):
    #将数据给到定义好的linear感知“鸡”中，对就是要 赶 只 鸡
    y_predict = linear(x_data)
    #获取loss
    loss = mse_loss(y_predict, y_data)
    #反向传播
    loss.backward()
    sgd_optimizer.step()
    sgd_optimizer.clear_grad()
    #获取感知“鸡”中的w1
    w1_after_opt = linear.weight.numpy()[0].item()
    #获取感知“鸡”中的w2
    w2_after_opt = linear.weight.numpy()[1].item()
    #获取感知“鸡”中的b
    b_after_opt = linear.bias.numpy().item()
    #每1000次输出一次数据
    if i%1000 == 0:
        print("epoch {} loss {}".format(i, loss.numpy()))
        print("w1 after optimize: {}".format(w1_after_opt))
        print("w2 after optimize: {}".format(w2_after_opt))
        print("b after optimize: {}".format(b_after_opt))
print("finished training， loss {}".format(loss.numpy()))

# 最后划线
plt.scatter(class1[:,0],class1[:,1])
plt.scatter(class2[:,0],class2[:,1],marker='*')
x=np.arange(10)
#画线的公式
y=-(w1_after_opt*x)/w2_after_opt+b_after_opt
plt.plot(x,y)
plt 

```


## 4、用朴素贝叶斯判断该不该嫁
> 如果一对男女朋友，男生想女生求婚，男生的四个特点分别是[**"不帅"，"性格不好"，"身高矮"，"不上进"**]，请你判断一下女生是嫁还是不嫁？


![image](https://ai-studio-static-online.cdn.bcebos.com/ebf7faae5b63406186d7e8b8cf353469c4184c49c3a2455398b5bbc495c15b2b)

参考： https://zhuanlan.zhihu.com/p/26262151


```python

# python演示代码
import numpy as  np
import pandas as pd

# 朴素贝叶斯
class Naive_Bayes:
    def __init__(self):
        pass

    # 朴素贝叶斯训练过程
    def nb_fit(self, X, y):
        classes = y[y.columns[0]].unique()
        class_count = y[y.columns[0]].value_counts()
        # 类先验概率
        class_prior = class_count / len(y)
        # 计算类条件概率
        prior = dict()
        for col in X.columns:
            for j in classes:
                p_x_y = X[(y == j).values][col].value_counts()
                for i in p_x_y.index:
                    prior[(col, i, j)] = p_x_y[i] / class_count[j]

        return classes, class_prior, prior

    # 预测新的实例
    def predict(self, X_test):
        res = []
        for c in classes:
            p_y = class_prior[c]
            p_x_y = 1
            for i in X_test.items():
                p_x_y *= prior[tuple(list(i) + [c])]
            res.append(p_y * p_x_y)
        return classes[np.argmax(res)]


if __name__ == "__main__":
    # 构建数据集
    # 帅不帅: 1 帅 0 不帅
    x_beautiful = [1,0,1,0,1,1,1,0,1,0,1,1]
    #性格: 1 好 0  不好
    x_character = [0,1,1,1,0,0,1,1,1,0,1,1]
    # 身高: 2 高 1 中 0  矮
    x_tall=[0,0,0,2,0,0,2,1,1,2,0,0]
    # 上进: 1 上进 0 不上进
    x_selfdrive=[0,1,1,1,1,1,0,1,1,1,0,0]

    # 嫁不嫁: 1 嫁  0 不嫁
    y = [0,0,1,1,0,0,1,1,1,1,0,0]

    df = pd.DataFrame({'x1': x_beautiful, 'x2': x_character,'x3': x_tall,'x4': x_selfdrive, 'y': y})
    X = df[['x1', 'x2', 'x3', 'x4']]
    y = df[['y']]

    # 测试数据:  不帅、性格不好、身高矮、不上进
    X_test = {'x1': 0, 'x2': 0, 'x3': 0, 'x4':0}

    # 构建朴素贝叶斯模型
    nb = Naive_Bayes()
    # 训练模型
    classes, class_prior, prior = nb.nb_fit(X, y)
    # 预测结果
    Y=nb.predict(X_test)
    print('[不帅、性格不好、身高矮、不上进]  预测 嫁/不嫁 的结果是：', '嫁' if  Y==1 else '不嫁'  )
    
    # 打印结果：
    # [不帅、性格不好、身高矮、不上进]  预测 嫁/不嫁 的结果是： 不嫁


```

[不帅、性格不好、身高矮、不上进]  预测 嫁/不嫁 的结果是： 不嫁


## 5、 用决策树进行贷款决策
> 贷款样本：
![](https://ai-studio-static-online.cdn.bcebos.com/0db9d12bdc0a4082978671ad2ba5e4f92ce4fecd55634eeca60561ae01ca899f)

![](https://ai-studio-static-online.cdn.bcebos.com/40d6c28d8b214f75935d1fe1f65338350560b364ff154be79224d4a9c8d6bdfc)

某中年人，有工作，没有房子，信贷情况好，是否该贷款给他呢？



```python
# Sklearn实现决策树
from sklearn import tree
import numpy as np
from sklearn.tree import export_graphviz
import graphviz

def loaddata():
    dataSet = [[0, 0,0,0, 'no'],
               [0, 0,0,1,'no'],
               [0, 1,0,1, 'yes'],
               [0, 1,1,0, 'yes'],
               [0, 0,0,0, 'no'],
               [1, 0,0,0, 'no'],
               [1, 0,0,1, 'no'],
               [1, 1,1,1, 'yes'],
               [1, 0,1,2, 'yes'],
               [1, 0,1,2, 'yes'],
               [2, 0,1,2, 'yes'],
               [2, 0,1,1, 'yes'],
               [2, 1,0,1, 'yes'],
               [2, 1,0,2, 'yes'],
               [2, 0,0,0,'no']]
    feature_name = ['age','job','house','credit']
    return dataSet, feature_name


myDat,feature_name = loaddata()
X = np.array(myDat)[:,0:4]
y = np.array(myDat)[:,-1]
model = tree.DecisionTreeClassifier()#DecisionTreeClassifier的常用参数见上
model.fit(X,y)
# 开始预测
print(model.predict([[1,1,0,1]]))


# 下面代码如果能正常运行，需：
# 需要提前装 graphviz 软件
# 1、下载graphviz安装包：http://www.graphviz.org/download/
# 2、在python中安装graphviz库，pip install graphviz

#导入生成好的决策树模型
dot_data = tree.export_graphviz(model
                                    ,out_file=None
                                    ,feature_names=feature_name
                                    ,class_names=['yes','no'] #标签的名称
                                    ,filled=True
                                    ,rounded=True)
graph = graphviz.Source(dot_data)
# graph.view()
graph

```


['yes']


参考： https://zhuanlan.zhihu.com/p/235304687


## 6、 单层感知机解数学方程
> 假设有二元一次数学方程： y=x1*1.2+x2*2.5+6.8
> 已经知道若干x1,x2和y的数值，求这个方程式中的参数变量



```python
import numpy

'''
基于PaddlePaddle2.0-构建线性回归模型
https://aistudio.baidu.com/aistudio/projectdetail/1632258

使用paddle实现单层感知机
https://aistudio.baidu.com/aistudio/projectdetail/1546134
'''

# 第一步 准备数据
num_inputs=2
num_examples=500
true_w=[1.2,2.5]
true_b=6.8
# 生成500组数据，每组数据有两个数字，代表两个特征
features = numpy.random.normal(0,1,(num_examples, num_inputs)).astype('float32')
print("查看输入的数据:")
# print(features)
#公式 y=x1*1.2+x2*2.5+6.8
labels = features[:,0]*true_w[0]+features[:,1]*true_w[1]+true_b
# 增加随机干扰的噪音
labels = labels + numpy.random.normal(0,0.001,labels.shape[0])
labels = labels.astype('float32')
# print(labels.shape,labels)
# axis=1代表扩展行，axis=0代表扩展列,axis=-1代表扩展最后一个参数：
labels = numpy.expand_dims(labels,axis=-1) #注意：需要在最后增加一个维度
print(labels.shape)

# 导入第三方库
# import paddle
# import numpy as np
# # 对数据进行定义
# x_data = paddle.to_tensor([[1., 2.], [2., 3.], [3., 4.], [4., 5.], [3., 5.], [2., 4.], [3., 4.]])
# y_data = paddle.to_tensor([[15.], [21.], [27.], [33.], [31.], [29.], [31.]])
# print(f'x_data:{x_data}')
# print(f'y_data:{y_data}')

# 第二步，构建线性回归模型
import paddle
train_data=paddle.to_tensor(features)
y_true=paddle.to_tensor(labels)
# 线性变换层
model=paddle.nn.Linear(in_features=2, out_features=1)

# 查看网络
print("查看网络")
paddle.summary(model, (2,))

# 第三步，构建优化器和损失函数
sgd_optimizer=paddle.optimizer.SGD(learning_rate=0.001, parameters=model.parameters())
mse_loss=paddle.nn.MSELoss()

# 第四步，训练模型
for i in range(5000):
    y_predict = model(train_data)
    # 均方损失
    loss=mse_loss(y_predict, y_true)
    # 反向传播
    loss.backward()
    # 执行一次优化器并进行参数更新
    sgd_optimizer.step()
    # 清除需要优化的参数的梯度
    sgd_optimizer.clear_grad()


# 查看训练结果以后的值：
print(model.weight.numpy())
print(model.bias.numpy())
print(loss.numpy())

# w1_before_opt = model.weight.numpy()[0].item()
# w2_before_opt = model.weight.numpy()[1].item()
# b_before_opt = model.bias.numpy().item()
# print("w1 before optimize: {}".format(w1_before_opt))
# print("w2 before optimize: {}".format(w2_before_opt))
# print("b before optimize: {}".format(b_before_opt))


```




## 7、 多项式模拟曲线
有一条S型曲线，求数学表达式

**自定义多元回归模型**

构建一个自定义的多元线性回归模型，来看看模型能否学习到我们事前自定义的参数。大体流程如下：
（1）自定义一些随机样本数据，然后事前设定多元回归模型参数（真实参数），采用该模型为每个样本生成真实标签。
（2）假设机器并不知道我们事先设定的真实参数，需要利用样本特征数据和真实标签来训练模型，通过随机梯度下降算法来更新和优化参数，这是参数估计过程。
（3）对比估计参数与我们事前设定的真实参数是否趋于一致。如果趋于一致，则表明模型通过学习样本，学习到了真实参数，反之就是没学到。

下面采用PaddlePaddle 2.0构建自定义线性回归模型。



```python

import paddle
import numpy as np
import os
import matplotlib
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")

print(paddle.__version__)

#定义函数
def poly_function(datain):
    temp = np.zeros(len(datain))
    for i in range(len(datain)):
        x = datain[i]
        temp[i] = x * x * x * x + 2 * x * x * x - 1 * x * x + 0.5 * x

    return temp

#准备训练集
data = np.random.uniform(-1, 1, 800)
orderdata = np.sort(data)
label = poly_function(orderdata)
orderdata = orderdata.reshape(800, 1)
label = label.reshape(800, 1)
datas = np.concatenate((orderdata, label), axis=1)

#定义函数
class Regressor(paddle.nn.Layer):
    def __init__(self):
        super(Regressor, self).__init__()
        self.fc = paddle.nn.Linear(1, 20)
        self.ac1 = paddle.nn.Sigmoid()
        self.fc2 = paddle.nn.Linear(20, 20)
        self.ac2 = paddle.nn.LeakyReLU()
        self.fc3 = paddle.nn.Linear(20, 1)

    def forward(self, inputs):
        x = self.fc(inputs)
        x = self.ac1(x)
        x = self.fc2(x)
        x = self.ac2(x)
        pred = self.fc3(x)
        return pred


train_nums = []
loss_seq = []
import paddle.nn.functional as F

#代码框架参考PaddlePaddle官方教程，波士顿房价预测
#训练
def train(model):
    model.train()
    BATCH_SIZE = 40
    EPOCH_NUM = 1000
    train_num = 0
    optimizer = paddle.optimizer.Adam(learning_rate=0.0002, parameters=model.parameters())
    for epoch_id in range(EPOCH_NUM):

        np.random.shuffle(datas)
        mini_batches = [datas[k: k + BATCH_SIZE] for k in range(0, len(datas), BATCH_SIZE)]
        for batch_id, data in enumerate(mini_batches):
            features_np = np.array(datas[:, 0], np.float32).reshape(800, 1)
            labels_np = np.array(datas[:, 1], np.float32).reshape(800, 1)
            features = paddle.to_tensor(features_np)

            labels = paddle.to_tensor(labels_np)
            # 前向计算
            y_pred = model(features)

            loss = F.mse_loss(y_pred, label=labels)
            loss_np = loss.detach().numpy()

            loss.backward()
            optimizer.step()
            optimizer.clear_grad()
            if batch_id % 30 == 0 and epoch_id % 50 == 0:
                print("Pass:%d,Cost:%0.5f" % (epoch_id, loss_np))
            train_num = train_num + BATCH_SIZE
            train_nums.append(train_num)
            loss_seq.append(loss_np)



model = Regressor()
train(model)
#准备测试集
test_data = np.random.uniform(-1, 1, 200)
test_data = np.sort(test_data)


def test(data, model):
    datanp = np.array(data, np.float32).reshape(len(data), 1)

    features = paddle.to_tensor(datanp)
    pred = model(features)
    # plt.plot(datanp, pred.detach().numpy(), color='red', label='training cost')
    return pred


def showresult(test_data):
    test_label = poly_function(test_data)
    pred = test(test_data, model)
    plt.clf()
    plt.plot(test_data, test_label, color='red', label='ground truth')
    plt.plot(test_data, pred.detach().numpy(), color='blue', label='predict')
    plt.legend()
    plt.savefig('result')
    plt.show()

if __name__ == '__main__':
    showresult(test_data)


```
