# 识别京东滑动框
> 基于paddledetection从零开始自定义数据集进行目标检测
![](https://ai-studio-static-online.cdn.bcebos.com/c98263a2c45e41399c2ea68d4ac5aa8d465b4d333dec446296b3b93bd0648198)

## 0. 环境检测
安装anaconda

安装cuda

先检查本机支持的GPU版本号
win+R打开cmd，输入nvidia-smi，即可看到支持的cuda版
![](https://ai-studio-static-online.cdn.bcebos.com/65083796e94e49df81caaa38161dbfd4cfb8a79298004495b09b74d812d52be3)


## 1. 安装PaddlePaddle

安装GPU版本

 GPU版本11.6

```
conda install paddlepaddle-gpu==2.3.2 cudatoolkit=11.6 -c https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/Paddle/ -c conda-forge 
```

 CUDA10.2

```
python -m pip install paddlepaddle-gpu==2.2.2 -i https://mirror.baidu.com/pypi/simple
```

如果没有GPU，则安装CPU版本
cpu版本

```
conda install paddlepaddle==2.3.2 --channel https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/Paddle/
python -m pip install paddlepaddle==2.2.2 -i https://mirror.baidu.com/pypi/simple
```

检测安装结果

在您的Python解释器中确认PaddlePaddle安装成功

```
import paddle
paddle.utils.run_check()
```

 确认PaddlePaddle版本
 在shell或者cmd中执行

```
python -c "import paddle; print(paddle.__version__)"
```

其他依赖安装
COCO-API:
运行需要COCO-API，安装方式如下：
 
> 安装pycocotools

```
pip install pycocotools
```

windows用户安装COCO-API方式：
1 注意如果是的winodw下安装时候是出现的安装错误 是应为缺少安装的 .Windows下没有c/c++代码编译环境
![](https://ai-studio-static-online.cdn.bcebos.com/9ba0323c1d9a486db144ff0b0eefaed6bd7757acbdc7435eb531e9beb31d6565)

解决办法：下载visualcppbuildtools_full.exe来安装 安装成功以后就会在开始菜单里找到visual C++ 2015 x64 Native Build Tools Command 如果暂时找不到就重启一下。

2.没有cython：解决办法：用anaconda下载就可以了

3.没有git： 解决办法：下载并按照指定步骤安装就可以


若Cython未安装，请安装Cython

```
pip install Cython
```
 
> 由于原版cocoapi不支持windows，采用第三方实现版本，该版本仅支持Python3
```
pip install git+https://github.com/philferriere/cocoapi.git#subdirectory=PythonAPI
```


## 2. 安装PaddleDetection

注意： pip安装方式只支持Python3
 克隆PaddleDetection仓库
```
cd <path/to/clone/PaddleDetection>
```

clone repository  国内的镜像，速度快
```
git clone https://gitee.com/PaddlePaddle/PaddleDetection.git
```

 国外的镜像，速度慢
```
git clone https://github.com/PaddlePaddle/PaddleDetection.git
```

 安装其他依赖
```
cd PaddleDetection
pip install -r requirements.txt

# 编译安装paddledet
python setup.py install
```

<html>
注意:
如果github下载代码较慢，可尝试使用gitee或者代理加速。
若您使用的是Windows系统，由于原版cocoapi不支持Windows，pycocotools依赖可能安装失败，可采用第三方实现版本，该版本仅支持Python3
</html>

```
pip install git+https://github.com/philferriere/cocoapi.git#subdirectory=PythonAPI
```

>若您使用的是Python <= 3.6的版本，安装pycocotools可能会报错distutils.errors.DistutilsError: Could not find suitable distribution for Requirement.parse('cython>=0.27.3'), 您可通过先安装cython解决该问题
```
pip install cython  
```

安装后确认测试通过：
```
python ppdet/modeling/tests/test_architectures.py
```

>测试通过后会提示如下信息：
```
.......
----------------------------------------------------------------------
Ran 7 tests in 12.816s
OK
```



## 3. 快速体验

 在GPU上预测一张图片
```
export CUDA_VISIBLE_DEVICES=0
```

  使用GPU
  利用在线的模型库进行预测
```
python tools/infer.py -c configs/ppyolo/ppyolo_r50vd_dcn_1x_coco.yml -o use_gpu=true weights=https://paddledet.bj.bcebos.com/models/ppyolo_r50vd_dcn_1x_coco.pdparams --infer_img=demo/000000014439.jpg
```

会在output文件夹下生成一个画有预测结果的同名图像。
结果如下图：

![](https://ai-studio-static-online.cdn.bcebos.com/16ff4b23303a41ed990e6e09d4806a76be90a86e7da2400389e3037cf75cad0a)


## 下面训练自己的模型

### 1、下面训练自己的模型

将图片都统一调整为正方形



![](https://github.com/szliszt/AI_Study_Notes_ByCase/raw/main/images/jdslider_pic9.png)


```python
import os
import math
import numpy as np
from scipy import misc, ndimage
import cv2
import matplotlib.pyplot as plt

def resize_pic(file_name,target_name):
    # 扩充图片的边界，调整为正方形
    # file_name="D:\PyProject\PaddleGPU37\data\JD\pic1.png"
    img = cv2.imdecode(np.fromfile(file_name, dtype=np.uint8), 1)  # 读成GRB

    '''
    扩充图像边界
    ? src 输入图像
    ? top, bottom, left, right 对应边界的像素数目。
    ? borderType 要添加那种类型的边界,类型如下
    – cv2.BORDER_CONSTANT 添加有颜色的常数值边界,还需要下一个参数(value)。
    – cv2.BORDER_REFLECT 边界元素的镜像。比如: fedcba|abcde-fgh|hgfedcb
    – cv2.BORDER_REFLECT_101 or cv2.BORDER_DEFAULT跟上面一样,但稍作改动。例如: gfedcb|abcdefgh|gfedcba
    – cv2.BORDER_REPLICATE 重复最后一个元素。例如: aaaaaa|abcdefgh|hhhhhhh
    – cv2.BORDER_WRAP 不知道怎么说了,就像这样: cdefgh|abcdefgh|abcdefg
    ? value 边界颜色,如果边界的类型是 cv2.BORDER_CONSTANT
    '''

    # yolo v3图片的输入尺寸是608
    target = 608

    # 原始图片大小
    width = 422
    height = 164

    # top,bottom,left,right
    # top=bottom=(target - height) / 2 = 222
    top=int((target-height)/2)
    bottom=top

    # left=right=(target-width)/2 =93
    left=int((target-width)/2)
    right=left

    # 多出的边界，统一补充128灰色
    dst = cv2.copyMakeBorder(img, top, bottom, left, right,
                           cv2.BORDER_CONSTANT, value=[128, 128, 128])
   
    cv2.imwrite( target_name , dst)

if __name__ == '__main__':
    #批量将图片扩充成正方形，多出来的位置设置为灰色
    # 从source拷贝到target
    for i in range(1,51):
        _filename="D:\PyProject\Mubiaojiance\data\jd\pic\source\pic{}.png".format(i)
        _targetname=_filename.replace("source","target")
        resize_pic(_filename,_targetname)

    print("ok")
    
```


**强烈建议将图片统一调整为:608*608大小(这是默认尺寸)**

用vott标注自己的数据集，格式为VOC
标注工具：
开箱即用
www.makesense.ai
微软的vott
https://github.com/Microsoft/VoTT/releases  
团队协作
cvat.org


在PaddleDetection\dataset目录下，新建文件夹jdslider
将标记好的文件 Annotations,JPEGImages 拷贝到该目录下 

2、数据扩充
```
!python  data_more.py
```

```python
# 数据扩展，增加数据集
# 图片文件复制多份
import shutil

import matplotlib.pyplot as plt # plt 用于显示图片
import matplotlib.image as mpimg # mpimg 用于读取图片
import numpy as np

# default_path = "/Users/lichunlei/PycharmProjects/RenGongZhiNeng/PaddleDetection/dataset"
default_path = ".."
project_path = "jdslider"

from os import listdir
from os.path import isfile, join
onlyfiles = [f for f in listdir(default_path) ]
# if isfile(join(default_path, f))
print("检测当前目录内容:")
print(onlyfiles)

import os
current_working_dir = os.getcwd()
print(f"当前路径为: {current_working_dir}")


import numpy as np
import cv2
from PIL import Image, ImageEnhance
import random

# 随机改变亮暗、对比度和颜色等
def random_distort(img):
    # 随机改变亮度
    def random_brightness(img, lower=0.5, upper=1.5):
        e = np.random.uniform(lower, upper)
        return ImageEnhance.Brightness(img).enhance(e)
    # 随机改变对比度
    def random_contrast(img, lower=0.5, upper=1.5):
        e = np.random.uniform(lower, upper)
        return ImageEnhance.Contrast(img).enhance(e)
    # 随机改变颜色
    def random_color(img, lower=0.5, upper=1.5):
        e = np.random.uniform(lower, upper)
        return ImageEnhance.Color(img).enhance(e)

    ops = [random_brightness, random_contrast, random_color]
    np.random.shuffle(ops)

    img = Image.fromarray(img)
    img = ops[0](img)
    img = ops[1](img)
    img = ops[2](img)
    img = np.asarray(img)

    return img

def batch_copy(from_index, to_index, skip_size):
    # skip_size 向后跳过数量，等于当前文件最大数量
    # 本批次复制的文件数
    to_index = to_index + 1

    for i in range(from_index, to_index):
        # print(i)
        old_file = "{}/{}/JPEGImages/pic{}.png".format(default_path, project_path, i)
        new_index =  skip_size+i
        new_file = "{}/{}/JPEGImages/pic{}.png".format(default_path, project_path, new_index)

        # 直接复制文件
        shutil.copyfile(old_file, new_file)

        print("read image from file {}".format(old_file))
        srcimg = Image.open(old_file)
        # 将PIL读取的图像转换成array类型
        srcimg = np.array(srcimg)
        # 对原图做 随机改变亮暗、对比度和颜色等 数据增强
        img_enhance = random_distort(srcimg)

        # img_enhance 是array
        # plt.imshow(img_enhance)  # 显示图片
        # 保存图片
        # plt.savefig(new_file)
        plt.imsave(new_file, img_enhance)
        # plt.image.imsave(new_file,img_enhance)
        # print("保存成功!")

        print("{}: random copy oldfile: {} to newfile: {}".format(new_index, old_file, new_file))

    for i in range(from_index, to_index):
        # print(i)
        old_file = "{}/{}/Annotations/pic{}.xml".format(default_path, project_path,i)
        new_index = i + skip_size
        new_file = "{}/{}/Annotations/pic{}.xml".format(default_path, project_path,new_index)
        shutil.copyfile(old_file, new_file)
        print("{}:  copy oldfile: {} to newfile: {}".format(new_index, old_file, new_file))


# batch_copy(1,50,50)
# # 将1-50复制一份，放在100以后
for i in range(50,2200,50):
    # 每次复制50份
    print("{}+（1,50）".format(i))
    batch_copy(1, 50, i)

# 统计有多少图像样本
import os
# images_path = "work/Annotations"
images_path = "./Annotations"
image_count = len([os.path.join(images_path, image_name)
          for image_name in os.listdir(images_path) if not image_name.startswith('.')])
print("用于训练的图片样本数量:", image_count)

```

### 3、划分数据集
```
!python divide_dataset.py
```

```python
# 划分数据集
import os
import random

'''
运行该代码将会生成trainval.txt、train.txt、val.txt、test.txt，将我们标注的600张图像按照训练集、验证集、测试集的形式做一个划分。
'''

# 划分数据集
import os
import random


# default_path="/home/aistudio/code/PaddleDetection/dataset/jdslider"
default_path="."

trainval_percent = 0.8  # 训练集验证集总占比
train_percent = 0.6  # 训练集在trainval_percent里的train占比
xmlfilepath =  "{}/Annotations".format(default_path)
txtsavepath = "{}/ImageSets/Main".format(default_path)
total_xml = os.listdir(xmlfilepath)

# print(total_xml)

num = len(total_xml)
list = range(num)
tv = int(num * trainval_percent)
tr = int(tv * train_percent)
trainval = random.sample(list, tv)
train = random.sample(trainval, tr)

ftrainval = open(r'{}/ImageSets/Main/trainval.txt'.format(default_path), 'w')
ftest = open(r'{}/ImageSets/Main/test.txt'.format(default_path), 'w')
ftrain = open(r'{}/ImageSets/Main/train.txt'.format(default_path), 'w')
fval = open(r'{}/ImageSets/Main/val.txt'.format(default_path), 'w')

print("总共有文件数:",num)
print("训练集验证集文件数:",tv)
print("训练集文件数:",tr)

for i in list:
    if total_xml[i].find("xml")>0:
    # if name.find("ipynb_checkpo")<0:
        name = total_xml[i][:-4] + '\n'
        if i in trainval:
            ftrainval.write(name)
            if i in train:
                ftrain.write(name)
            else:
                fval.write(name)
        else:
            ftest.write(name)

ftrainval.close()
ftrain.close()
fval.close()
ftest.close()

print("ok")

```

### 4、建立位置索引
```
!python create_list.py 
```

```python
import os
import re
import random

'''
可根据在Main文件夹中划分好的数据集进行位置索引，生成含有图像及对应的XML文件的地址信息的文件。
'''

devkit_dir = './'
output_dir = './'

# devkit_dir = '/home/aistudio/code/PaddleDetection/dataset/jdslider/'
# output_dir = '/home/aistudio/code/PaddleDetection/dataset/jdslider/'

def get_dir(devkit_dir,  type):
    return os.path.join(devkit_dir, type)

def walk_dir(devkit_dir):
    filelist_dir = get_dir(devkit_dir, 'ImageSets/Main')
    annotation_dir = get_dir(devkit_dir, 'Annotations')
    img_dir = get_dir(devkit_dir, 'JPEGImages')
    trainval_list = []
    train_list = []
    val_list = []
    test_list = []

    print(filelist_dir)

    added = set()

    for _, _, files in os.walk(filelist_dir):
        for fname in files:
            print(fname)
            img_ann_list = []
            if re.match('trainval.txt', fname):
                img_ann_list = trainval_list
            elif re.match('train.txt', fname):
                img_ann_list = train_list
            elif re.match('val.txt', fname):
                img_ann_list = val_list
            elif re.match('test.txt', fname):
                img_ann_list = test_list
            else:
                continue
            fpath = os.path.join(filelist_dir, fname)
            for line in open(fpath):
                name_prefix = line.strip().split()[0]
                print(name_prefix)

                added.add(name_prefix)
                #ann_path = os.path.join(annotation_dir, name_prefix + '.xml')
                ann_path = annotation_dir + '/' + name_prefix + '.xml'
                print(ann_path)
                #img_path = os.path.join(img_dir, name_prefix + '.jpg')
                img_path = img_dir + '/' + name_prefix + '.png'
                assert os.path.isfile(ann_path), 'file %s not found.' % ann_path
                assert os.path.isfile(img_path), 'file %s not found.' % img_path
                img_ann_list.append((img_path, ann_path))
            print(img_ann_list)

    return trainval_list, train_list, val_list, test_list


def prepare_filelist(devkit_dir, output_dir):
    trainval_list = []
    train_list = []
    val_list = []
    test_list = []

    trainval, train, val, test = walk_dir(devkit_dir)

    trainval_list.extend(trainval)
    train_list.extend(train)
    val_list.extend(val)
    test_list.extend(test)
    #print(trainval)
    with open(os.path.join(output_dir, 'trainval.txt'), 'w') as ftrainval:
        for item in trainval_list:
            ftrainval.write(item[0] + ' ' + item[1] + '\n')

    with open(os.path.join(output_dir, 'train.txt'), 'w') as ftrain:
        for item in train_list:
            ftrain.write(item[0] + ' ' + item[1] + '\n')

    with open(os.path.join(output_dir, 'val.txt'), 'w') as fval:
        for item in val_list:
            fval.write(item[0] + ' ' + item[1] + '\n')

    with open(os.path.join(output_dir, 'test.txt'), 'w') as ftest:
        for item in test_list:
            ftest.write(item[0] + ' ' + item[1] + '\n')


if __name__ == '__main__':
    prepare_filelist(devkit_dir, output_dir)
    
```

### 5、参数预处理
```
!python cal_parameter.py
```

```python
# -*- coding=utf-8 -*-
# 利用k-means聚类计算九个先验锚框

# 首先使用K-means计算最适合的anchors大小
# 由于voc数据集的标注文件都是xml格式的，因此使用ET模块解析xml标注文件
import xml.etree.ElementTree as ET
import numpy as np
import os


# 目标图像大小
tar_size = 608
# tar_size = 422

# 重新处理每个图像中所有锚框的宽高
def load_one_info(name):
    filename = os.path.join(base, 'Annotations', name)

    tree = ET.parse(filename)
    size = tree.find('size')
    width = float(size.find('width').text)
    height = float(size.find('height').text)
    ratio = min(tar_size / width, tar_size / height)

    Objects = tree.findall('object')
    # 确定图像中有多少个目标
    objs_num = len(Objects)
    # 初始化每个目标的锚框，初始化为零
    Boxes = np.zeros((objs_num, 4), dtype=np.float32)
    # 初始化图像的类别标签
    True_classes = np.zeros((objs_num), dtype=np.float32)

    result = []
    for i, obj in enumerate(Objects):
        bbox = obj.find('bndbox')

        # 计算每个锚框的坐标
        x_min = float(bbox.find('xmin').text) - 1
        y_min = float(bbox.find('ymin').text) - 1
        x_max = float(bbox.find('xmax').text) - 1
        y_max = float(bbox.find('ymax').text) - 1

        # 由于图像进行了resize，这里需要重新计算改变之后的锚框的位置
        w = ratio * (x_max - x_min)
        h = ratio * (y_max - y_min)
        # 这个函数返回的是锚框的宽和高，就是为了处理锚框在resize后的大小
        result.append([w, h])

    return result


def iou(box, clusters):
    """
    计算锚框box与每个簇心的交并比
    """
    x = np.minimum(clusters[:, 0], box[0])
    y = np.minimum(clusters[:, 1], box[1])
    if np.count_nonzero(x == 0) > 0 or np.count_nonzero(y == 0) > 0:
        return 0
        raise ValueError("Box has no area")

    # 计算锚框的面积、簇心的面积和交集的面积
    intersection = x * y
    box_area = box[0] * box[1]
    cluster_area = clusters[:, 0] * clusters[:, 1]

    # 计算交并比
    iou_ = np.true_divide(intersection, box_area +
                          cluster_area - intersection + 1e-10)
    # iou_ = intersection / (box_area + cluster_area - intersection + 1e-10)

    return iou_


def avg_iou(boxes, clusters):
    # 计算所有锚框交并比的平均值
    return np.mean([np.max(iou(boxes[i], clusters)) for i in range(boxes.shape[0])])


def translate_boxes(boxes):

    new_boxes = boxes.copy()
    for row in range(new_boxes.shape[0]):
        new_boxes[row][2] = np.abs(new_boxes[row][2] - new_boxes[row][0])
        new_boxes[row][3] = np.abs(new_boxes[row][3] - new_boxes[row][1])
    return np.delete(new_boxes, [0, 1], axis=1)


def kmeans(boxes, k, dist=np.median):
    # 计算总共有多少个锚框（40138个）
    rows = boxes.shape[0]
    # 产生一个rows行，k列的随机未初始化的数组
    distances = np.empty((rows, k))
    # 产生一个全零向量
    last_clusters = np.zeros((rows,))
    # 随机种子(并未设置种子，因此此行代码不起作用)
    np.random.seed()

    # 从0-rows中随机选择k个数字，不允许相同，然后这个k个数字作为索引对应的锚框选出来存入clusters数组作为初始簇心
    clusters = boxes[np.random.choice(rows, k, replace=False)]

    while True:
        for row in range(rows):
            # 遍历每一个锚框与每个簇心的iou存入distances
            distances[row] = 1 - iou(boxes[row], clusters)

        # 计算出每个锚框最接近的簇心的序号(0-8)并存入一维数组（40138个）
        nearest_clusters = np.argmin(distances, axis=1)
        # 如果每个锚框所属的簇不发生变化，那么停止迭代
        if (last_clusters == nearest_clusters).all():
            break

        # 否则需要继续更新簇心
        for cluster in range(k):
            # 将所有属于某一个簇的锚框的下标都找到，计算这些锚框的中心点得到新的簇心
            clusters[cluster] = dist(boxes[nearest_clusters == cluster], axis=0)

        # 更新目前每个锚框所属于的簇
        last_clusters = nearest_clusters

    return clusters


def get_kmeans(anno, cluster_num=9):
    # 计算出聚类之后的簇心
    anchors = kmeans(anno, cluster_num)
    # 计算簇心的平均交并比
    ave_iou = avg_iou(anno, anchors)

    anchors = anchors.astype('int').tolist()
    # 按照锚框的面积对锚框排序
    anchors = sorted(anchors, key=lambda x: x[0] * x[1])

    return anchors, ave_iou


# 通过k-means聚类计算最适合的anchor尺寸(九个簇的中心)
result = []  # 存储每个图像所有anchor宽高的集合(每个图像可能包含若干个二维数组)
# base = 'PaddleDetection/dataset/VOC2007'
base = 'D:\PyProject\Mubiaojiance\PaddleDetection\dataset\jdslider'
for name in os.listdir(os.path.join(base, 'Annotations')):
    if name == ".ipynb_checkpoints":
        continue
    result.extend(load_one_info(name))

result = np.array(result)
# 聚类计算九个簇的中心和平均iou
anchors, ave_iou = get_kmeans(result, 9)

anchor_string = ''
anchor_sizes = []
# 打印聚类得到的锚框簇心(共九个)
for anchor in anchors:
    anchor_string += '{},{}, '.format(anchor[0], anchor[1])
    anchor_sizes.append([anchor[0], anchor[1]])
anchor_string = anchor_string[:-2]

print('anchors are:')
print(anchor_string)
print('the average iou is:')
print(ave_iou)

```

**系统自带的计算锚框工具：**

python tools/anchor_cluster.py -c configs/yolov3/yolov3_mobilenet_v1_270e_voc_jd.yml -n 9 -s 608 -m v2 -i 1000

- -n Anchor的数目
- -s 图像尺寸大小，若指定，则使用指定的尺寸，如果不指定, 则尝试从配置文件中读取图片尺寸。这里默认应该设置为608
- -m 使用的Anchor聚类方法,目前只支持yolov2/v5的聚类算法
- -i/–iters  kmeans算法收敛或者达到迭代次数后终止  



### 6、修改配置文件的参数

**注意：配置文件中不能有汉字！**

- epoch：训练轮次(纪元数)，每经过一个epochs所有的训练集数据都在网络中计算了一遍
- batch_size：批尺寸，由于无法一次性把所有的训练集同时输入网络训练，所以需要分批训练，每批数据样本的个数即为batch_size，配置文件中batch_size=8表示每个取8个训练集图片开始训练
- max_iters：最大训练轮数，每经过一轮iter，就会取batch_size个样本进行训练（配置文件中需要设定）

- 例如batch_size=8，训练集数量为593，如果要要在单卡GUP上训练36个Epoch，那么max_iters=593x36/8=2669。（因此当max_iters = 2669时，训练了36epochs，如果希望增大训练epochs，则按比例增加max_iters即可）
- max_iters=img_num*Epoch/batch_size
- 学习率的milestones（学习率变化界限）一般与max_iters存在比例关系，一般在总max_iters数的2/3和8/9处进行学习率的调整
- 为保证模型正常训练不出Nan，学习率要根据GPU卡数，batch size变换而做线性变换，比如这里我们将GPU卡数8->1，所以base_lr除以8即可；（这里也可以为了收敛更快，base_lr除以了4） 
- 学习率参考：https://github.com/PaddlePaddle/PaddleDetection/blob/release/0.4/docs/FAQ.md#faq%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98


修改训练轮数与学习率等参数：
根据训练集数量与总batch_size大小计算epoch数，然后将epoch数换算得到训练总轮数max_iters。
milestones（学习率变化界限）也是同理。
原配置文件中总batch_size=2*8=16（8卡训练），训练集数量约为12万张，max_iters=90000，所以epoch数=16x90000/120000=12。

在AI识虫数据集中，训练集数量约为1700，在单卡GPU上训练，max_iters=12x1700/2=10200。同理计算milestones为: [6800, 9000]。

学习率与GPU数量呈线性变换关系，如果GPU数量减半，那么学习率也将减半。
由于PaddleDetection中的faster_rcnn_r50_fpn模型是在8卡GPU环境下训练得到的，所以我们要将学习率除以8：


configs/yolov3/_base_/optimizer_270e.yml 中 epoch 修改为100， 减少PiecewiseDecay的milestones到60和80，由于使用单卡训练，将LearningRate.base_lr减少8倍到0.000125。




yolov3_mobilenet_v1_270e_voc_jd.yml
复制D:\PyProject\Mubiaojiance\PaddleDetection\configs\yolov3\yolov3_mobilenet_v1_270e_voc.yml 
生成
D:\PyProject\Mubiaojiance\PaddleDetection\configs\yolov3\yolov3_mobilenet_v1_270e_voc_jd.yml
修改参数:
snapshot_epoch，base_lr，milestones

```

_BASE_: [
  '../datasets/jd_voc.yml',  # 设置数据集
  '../runtime.yml',       #  公共运行参数，否使用GPU、每多少个epoch存储checkpoint等
  '_base_/optimizer_270e.yml',    # 设置优化器 epoch 学习率等
  '_base_/yolov3_mobilenet_v1.yml',  # 设置网络结构，models配置
  '_base_/yolov3_reader.yml',    # 设置加载数据尺寸、batch_size等
]


snapshot_epoch: 5  # 模型保存间隔，如果训练时eval设置为True，会在保存后进行验证
weights: output/yolov3_mobilenet_v1_270e_voc/model_final   # 训练权重的保存路径

# set collate_batch to false because ground-truth info is needed
# on voc dataset and should not collate data in batch when batch size
# is larger than 1.
EvalReader:
  collate_batch: false

LearningRate:
  base_lr: 0.001   #这里要修改
  schedulers:
  - !PiecewiseDecay
    gamma: 0.1    #衰减系数
    milestones:   #衰减点[列表]  #这里也要修改
    - 216
    - 243
  - !LinearWarmup   #学习率从非常小的数值线性增加到预设值之后，然后再线性减小。
    start_factor: 0.
    steps: 1000
 
 
 ```
 
 jd_voc.yml
复制文件D:\PyProject\Mubiaojiance\PaddleDetection\configs\datasets\voc.yml
生成
D:\PyProject\Mubiaojiance\PaddleDetection\configs\datasets\jd_voc.yml
修改参数：
num_classes，

```
metric: VOC
map_type: integral
num_classes: 1  # 有几种分类，如果只检测一个目标，那么就是
# 注意：在faster-rcnn训练的时候，在数据集合中的使用默认把背景当做一个类别。这样的是的需要在的原来的类别的基础上+1

TrainDataset:
  !VOCDataSet
    dataset_dir: dataset/jdslider
    anno_path: train.txt
    label_list: label_list.txt
    data_fields: ['image', 'gt_bbox', 'gt_class', 'difficult']

EvalDataset:
  !VOCDataSet
    dataset_dir: dataset/jdslider
    anno_path: val.txt
    label_list: label_list.txt
    data_fields: ['image', 'gt_bbox', 'gt_class', 'difficult']

TestDataset:
  !ImageFolder
    anno_path: dataset/jdslider/label_list.txt


```


修改D:\PyProject\Mubiaojiance\PaddleDetection\configs\runtime.yml

修改参数：
use_gpu，log_iter，snapshot_epoch

```
use_gpu: true   # if use cpu,set false !
use_xpu: false
log_iter: 20  # 训练日志输出间隔（单位：迭代次数），迭代多少次打印一次。值越小，打印的越频繁
save_dir: output
snapshot_epoch: 10     # 每隔 10个epoch eval 一次 
print_flops: false

# Exporting the model
export:
  post_process: True  # Whether post-processing is included in the network when export model.
  nms: True           # Whether NMS is included in the network when export model.
  benchmark: False    # It is used to testing model performance, if set `True`, post-process and NMS will not be exported.
  fuse_conv_bn: False
  
  
 ```
 
 
 修改 D:\PyProject\Mubiaojiance\PaddleDetection\configs\yolov3\_base_\optimizer_270e.yml
修改参数：epoch，base_lr，milestones

```
epoch: 270

LearningRate:
     #实例化学习率
                    # 初始学习率, 一般情况下8卡gpu，batch size为2时设置为0.02
                    # 可以根据具体情况，按比例调整
                    # 比如说4卡V100，bs=2时，设置为0.01
  base_lr: 0.001  # 学习率
  # if epoch < 216:
    #    learning_rate = 0.1
    # elif 216 <= epoch < 243:
    #    learning_rate = 0.1 * 0.1
    # else:
    #    learning_rate = 0.1 * (0.1)**2


  schedulers: #实例化优化器策略
  - !PiecewiseDecay #分段式衰减
    gamma: 0.1  #衰减系数
    milestones:  
    - 216  #在epoch为216时学习率衰减一次,270*0.8
    - 243  #在epoch为243时学习率衰再减一次,270*0.9
     # 在训练开始时，调低学习率为base_lr * start_factor，然后逐步增长到base_lr，这个过程叫学习率热身，按照以下公式更新学习率
                    # linear_step = end_lr - start_lr
                    # lr = start_lr + linear_step * (global_step / warmup_steps)
                    # 具体实现参考[API](fluid.layers.linear_lr_warmup)
  - !LinearWarmup   # #学习率从非常小的数值线性增加到预设值之后，然后再线性减小。
    start_factor: 0.
    steps: 4000    #线性增长步长

OptimizerBuilder:  #构建优化器
  optimizer:
    momentum: 0.9  #动量系数
    type: Momentum #类型
  regularizer:
    factor: 0.0005  #正则系数
    type: L2        #L2正则
    
    
   ```
   
   
   修改：D:\PyProject\Mubiaojiance\PaddleDetection\configs\yolov3\_base_\yolov3_mobilenet_v1.yml
修改参数：anchors


```
architecture: YOLOv3
pretrain_weights: https://paddledet.bj.bcebos.com/models/pretrained/MobileNetV1_pretrained.pdparams
norm_type: sync_bn

YOLOv3:
  backbone: MobileNet # YOLOv3的backbone网络，取值范围为[‘DarkNet53’, ‘ResNet34’, ‘MobileNetV1’, ‘MobileNetV3_large’]。默认为’MobileNetV1’
  neck: YOLOv3FPN
  yolo_head: YOLOv3Head
  post_process: BBoxPostProcess

MobileNet:
  scale: 1
  feature_maps: [4, 6, 13]
  with_extra_blocks: false
  extra_block_filters: []

# use default config
# YOLOv3FPN:

YOLOv3Head:
#  anchors: [[10, 13], [16, 30], [33, 23],
#            [30, 61], [62, 45], [59, 119],
#            [116, 90], [156, 198], [373, 326]]
#  anchors: [ [ 84,82 ], [ 84,83 ], [ 83,84 ],
#             [ 85,83 ], [84,84 ], [ 85,84 ],
#             [ 84,85 ], [ 85,85 ], [ 85,86 ] ]

# anchor框的宽度和高度，为None时表示使用默认值 
# [[10, 13], [16, 30], [33, 23], [30, 61], [62, 45], [59, 119], [116, 90], [156, 198], [373, 326]]

  anchors: [ [ 59,57 ], [ 58,58 ], [ 58,58 ],
             [ 58,59 ], [ 59,58 ], [ 58,59 ],
             [ 59,59 ], [ 59,59 ], [ 59,60 ] ]

  anchor_masks: [[6, 7, 8], [3, 4, 5], [0, 1, 2]]  # 在计算YOLOv3损失时，使用anchor的mask索引
  loss: YOLOv3Loss

YOLOv3Loss:
  ignore_thresh: 0.7 # #正例阈值。在计算YOLOv3损失时，IoU大于ignore_threshold的预测框的置信度被忽略
  downsample: [32, 16, 8] # #下采样倍数
  label_smooth: false

BBoxPostProcess:
  decode:
    name: YOLOBox
    conf_thresh: 0.005
    downsample_ratio: 32
    clip_bbox: true
  nms:
    name: MultiClassNMS  #  nms 类型参数，可以设置为[MultiClassNMS, MultiClassSoftNMS, MatrixNMS], 默认使用 MultiClassNMS
    keep_top_k: 100    #bbox最大个数
    score_threshold: 0.01 #置信度阈值
    nms_threshold: 0.45 #nms阈值
    nms_top_k: 1000 #nms最大框个数
    
 ```
 
 
 
 修改：D:\PyProject\Mubiaojiance\PaddleDetection\configs\yolov3\_base_\yolov3_reader.yml
修改参数：batch_size，mixup_epoch

```
worker_num: 2   # 每张GPU reader进程个数, #数据读取线程数
TrainReader:
  inputs_def:
    num_max_boxes: 50  # num_max_boxes，每个样本的groud truth的最多保留个数，若不够用0填充。
  sample_transforms:  #单张图片数据前处理，数据增强，下面是各种数据增强方法，放入列表中
    - Decode: {}
    - Mixup: {alpha: 1.5, beta: 1.5}
    - RandomDistort: {}
    - RandomExpand: {fill_value: [123.675, 116.28, 103.53]}
    - RandomCrop: {}
    - RandomFlip: {}
  batch_transforms:
      # 多尺度训练时，从list中随机选择一个尺寸，对一个batch数据同时同时resize
    - BatchRandomResize: {target_size: [320, 352, 384, 416, 448, 480, 512, 544, 576, 608], random_size: True, random_interp: True, keep_ratio: False}
    - NormalizeBox: {}
    - PadBox: {num_max_boxes: 50}
    - BboxXYXY2XYWH: {}
    - NormalizeImage: {mean: [0.485, 0.456, 0.406], std: [0.229, 0.224, 0.225], is_scale: True}
    - Permute: {}
    - Gt2YoloTarget: {anchor_masks: [[6, 7, 8], [3, 4, 5], [0, 1, 2]], anchors: [[10, 13], [16, 30], [33, 23], [30, 61], [62, 45], [59, 119], [116, 90], [156, 198], [373, 326]], downsample_ratios: [32, 16, 8]}
  batch_size: 1  # 训练数据batch大小,1个GPU的batch size，默认为1。需要注意：每个iter迭代会运行batch_size * device_num张图
  shuffle: true  # 是否shuffle
  drop_last: true # 注意，在某些情况下，drop_last=false时训练过程中可能会出错，建议训练时都设置为true
  mixup_epoch: 250   # mixup -1表示不做Mixup数据增强。注意，这里是epoch为单位
  use_shared_memory: true

EvalReader:
  inputs_def:
    num_max_boxes: 50
  sample_transforms:
    - Decode: {}
    - Resize: {target_size: [608, 608], keep_ratio: False, interp: 2}
    - NormalizeImage: {mean: [0.485, 0.456, 0.406], std: [0.229, 0.224, 0.225], is_scale: True}
    - Permute: {}
  batch_size: 1

TestReader:
  inputs_def:
    image_shape: [3, 608, 608]   #输入图片大小
  sample_transforms:
    - Decode: {}
    - Resize: {target_size: [608, 608], keep_ratio: False, interp: 2}
    - NormalizeImage: {mean: [0.485, 0.456, 0.406], std: [0.229, 0.224, 0.225], is_scale: True}
    - Permute: {}
  batch_size: 1
  
  
 ```
 
### 7、训练模型


```
# set PYTHONPATH=$PYTHONPATH:.

# 使用cpu
python tools/train.py -c configs/yolov3/yolov3_mobilenet_v1_270e_voc_jd.yml  --use_vdl=true --vdl_log_dir=vdl_dir/scalar  --eval

# 使用GPU （默认只有一个GPU） 
set CUDA_VISIBLE_DEVICES=0  #windows和Mac下不需要执行该命令
python tools/train.py -c configs/yolov3/yolov3_mobilenet_v1_270e_voc_jd.yml  --use_vdl=true --vdl_log_dir=vdl_dir/scalar -o use_gpu=true

```
关于训练命令的阐述：
-c configs/yolov3_mobilenet_v1_jd.yml   用来指定配置文件
--use_tb 是否使用tb-paddle记录数据，进而在TensorBoard中显示，默认值是False
--tb_log_dir 指定 tb-paddle 记录数据的存储路径gp
--eval 是否边训练边测试
关于–eval参数的使用：
在训练中交替执行评估, 评估在每个snapshot_iter时开始。每次评估后还会评出最佳mAP模型保存到best_model文件夹下，建议训练时候使用该参数，可以使得完成训练后快速找到最好的模型。



### 8、查看训练进度

```
# 切换环境
conda activate mubiaojiance 
# 注意切换路径
# cd paddledetection
# visualdl --logdir vdl_dir/scalar/  --port 8810

# 如果还是看不到图像，请用绝对路径
visualdl --logdir=D:\PyProject\Mubiaojiance\PaddleDetection\vdl_dir\scalar 

```



### 9、模型导出

```
python tools/export_model.py -c configs/yolov3/yolov3_mobilenet_v1_270e_voc_jd.yml --output_dir=./inference_model -o weights=output/yolov3_mobilenet_v1_270e_voc_jd/model_final
```


### 10、模型评估

```
python tools/eval.py -c configs/yolov3/yolov3_mobilenet_v1_270e_voc_jd.yml -o weights=output/yolov3_mobilenet_v1_270e_voc_jd/model_final
```
### 11、模型预测：

```
python tools/infer.py -c configs/yolov3/yolov3_mobilenet_v1_270e_voc_jd.yml -o use_gpu=true weights=output/yolov3_mobilenet_v1_270e_voc_jd/model_final.pdparams  --infer_img=demo/pic4.png --save_results=true
```

### 12、模型部署 
安装类库

```
pip install fastdeploy
# cpu版本
pip install numpy opencv-python fastdeploy-python -f https://www.paddlepaddle.org.cn/whl/fastdeploy.html

#gpu版本
#同步安装numpy,opencv-python
pip install numpy opencv-python fastdeploy-gpu-python -f https://www.paddlepaddle.org.cn/whl/fastdeploy.html

# 仅安装fastdeploy
# pip install   fastdeploy-gpu-python -f https://www.paddlepaddle.org.cn/whl/fastdeploy.html

# Conda安装(推荐)
conda config --add channels conda-forge && conda install cudatoolkit=11.2 cudnn=8.2


```

推理代码

```
import cv2
import fastdeploy.vision as vision

model = vision.detection.YOLOv3("inference_model/yolov3_mobilenet_v1_270e_voc_jd/model.pdmodel",
                                 "inference_model/yolov3_mobilenet_v1_270e_voc_jd/model.pdiparams",
                                 "inference_model/yolov3_mobilenet_v1_270e_voc_jd/infer_cfg.yml")
im = cv2.imread("demo/pic4.png")
result = model.predict(im.copy())
print(result)

vis_im = vision.vis_detection(im, result, score_threshold=0.5)
cv2.imwrite("demo/pic4_test.png", vis_im)


# 参考 https://github.com/PaddlePaddle/FastDeploy

```

  
