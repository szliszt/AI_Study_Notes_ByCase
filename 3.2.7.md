# 根据商品名称自动匹配sku 

## 项目背景：
开发票的时候，商品名称书写不规范，人工匹配条码费时费力。比如商品名称叫  iphone_plus_玫瑰金_128G，发票可能开成 苹果手机(plus)_玫瑰金/128G
## 项目目标：
喂给模型商品名称和SKU，然后给一个写法不同但是意思相同的商品名称，自动识别出SKU

## 实现代码 

### 1、下载数据 

从https://aistudio.baidu.com/aistudio/datasetdetail/135131 

下载商品档案 standard_product_library_535543_20220331.csv


### 2、环境准备 

```python
# 安装类库:
!pip install paddlepaddle==2.3.1
# conda install sqlalchemy
# conda install psycopg2
!pip install paddlenlp==2.3.4
!pip install paddlehub==2.3.0
```


```python
from flask import Flask, request, jsonify

import paddle
import paddle.nn.functional as F
from paddlenlp.datasets import load_dataset
import paddlenlp
import pandas as pd
import numpy as np
import re
import json

from paddlenlp.embeddings import TokenEmbedding
from paddlenlp import Taskflow
import paddlehub as hub
import paddle.nn as nn
import pandas as pd
import time

from urllib.parse import  unquote
import urllib
from urllib import parse

# paddle.enable_static()
paddle.disable_static()

```

```python
app = Flask(__name__)
# 解决中文乱码的问题
app.config['JSON_AS_ASCII'] = False

```


### 3、数据准备 

```python

filename="data/data135131/standard_product_library_535543_20220331.csv"

# 读取完整的数据集，通过品牌筛选可以大幅度提高计算速度。
# 类似地，也可以通过筛选品类，规格型号来加快匹配速度 
df=pd.read_csv(filename)[["行业商品编码","行业商品名称","商品品牌"]]
# 清理掉没有品牌的商品
df=df[~df["商品品牌"].isnull()]
df["行业商品名称"]=df["行业商品名称"].map(lambda x: x.replace("\n","")) # 替换中间的换行符
# 字段重命名
df.columns=["sku","title","brand"]
print("商品记录数:",df.shape[0])
print(df.head())

brand_list=df["brand"].unique().tolist()
# 以降序对列表进行排序-长度
brand_list.sort(key=len,reverse=True)
print("品牌列表:",len(brand_list))
print(brand_list[:10])

```

### 4、粗排算法（比较粗略的相似度计算：余弦相似度）

```python

# Word2vec 有两种词向量学习方式：CBOW 和Skip-Gram
# 本例中使用 CBOW 学习词向量，计算粗略的文本相似度

# 加载TokenEmbedding
# 初始化TokenEmbedding， 预训练embedding未下载时会自动下载并加载数据
token_embedding = TokenEmbedding(embedding_name="w2v.baidu_encyclopedia.target.word-word.dim300")

class BoWModel(nn.Layer):
    def __init__(self, embedder):
        super().__init__()
        self.embedder = embedder
        emb_dim = self.embedder.embedding_dim
        self.encoder = paddlenlp.seq2vec.BoWEncoder(emb_dim)
        self.cos_sim_func = nn.CosineSimilarity(axis=-1)

    def get_cos_sim(self, text_a, text_b):
        text_a_embedding = self.forward(text_a)
        text_b_embedding = self.forward(text_b)
        cos_sim = self.cos_sim_func(text_a_embedding, text_b_embedding)
        return cos_sim

    def forward(self, text):
        # Shape: (batch_size, num_tokens, embedding_dim)
        embedded_text = self.embedder(text)

        # Shape: (batch_size, embedding_dim)
        summed = self.encoder(embedded_text)

        return summed

model = BoWModel(embedder=token_embedding)

# 构造Tokenizer
'''
Tokenizer 的作用是分词。
jieba 中文分词工具
调用 jieba 分词工具，通过 API 将 token_embedding 的字典加载进去
'''
from collections import defaultdict

import numpy as np
import jieba
import paddle
from paddlenlp.data import JiebaTokenizer, Pad, Stack, Tuple, Vocab


class Tokenizer(object):
    def __init__(self):
        self.vocab = {}
        self.tokenizer = jieba
        self.vocab_path = 'vocab.txt'
        self.UNK_TOKEN = '[UNK]'
        self.PAD_TOKEN = '[PAD]'

    def set_vocab(self, vocab):
        self.vocab = vocab
        self.tokenizer = JiebaTokenizer(vocab)

    def build_vocab(self, sentences):
        word_count = defaultdict(lambda: 0)
        for text in sentences:
            words = jieba.lcut(text)
            for word in words:
                word = word.strip()
                if word.strip() != '':
                    word_count[word] += 1

        word_id = 0
        for word, num in word_count.items():
            if num < 5:
                continue
            self.vocab[word] = word_id
            word_id += 1

        self.vocab[self.UNK_TOKEN] = word_id
        self.vocab[self.PAD_TOKEN] = word_id + 1
        self.vocab = Vocab.from_dict(self.vocab,
                                     unk_token=self.UNK_TOKEN, pad_token=self.PAD_TOKEN)
        # dump vocab to file
        self.dump_vocab(self.UNK_TOKEN, self.PAD_TOKEN)
        self.tokenizer = JiebaTokenizer(self.vocab)
        return self.vocab

    def dump_vocab(self, unk_token, pad_token):
        with open(self.vocab_path, "w", encoding="utf8") as f:
            for word in self.vocab._token_to_idx:
                f.write(word + "\n")

    def text_to_ids(self, text):
        input_ids = []
        unk_token_id = self.vocab[self.UNK_TOKEN]
        for token in self.tokenizer.cut(text):
            token_id = self.vocab.token_to_idx.get(token, unk_token_id)
            input_ids.append(token_id)

        return input_ids

    def convert_example(self, example, is_test=False):
        input_ids = self.text_to_ids(example['text'])

        if not is_test:
            label = np.array(example['label'], dtype="int64")
            return input_ids, label
        else:
            return input_ids


def create_dataloader(dataset,
                      trans_fn=None,
                      mode='train',
                      batch_size=1,
                      pad_token_id=0):
    """
    Creats dataloader.
    Args:
        dataset(obj:`paddle.io.Dataset`): Dataset instance.
        mode(obj:`str`, optional, defaults to obj:`train`): If mode is 'train', it will shuffle the dataset randomly.
        batch_size(obj:`int`, optional, defaults to 1): The sample number of a mini-batch.
        pad_token_id(obj:`int`, optional, defaults to 0): The pad token index.
    Returns:
        dataloader(obj:`paddle.io.DataLoader`): The dataloader which generates batches.
    """
    if trans_fn:
        dataset = dataset.map(trans_fn, lazy=True)

    shuffle = True if mode == 'train' else False
    sampler = paddle.io.BatchSampler(
        dataset=dataset, batch_size=batch_size, shuffle=shuffle)
    batchify_fn = lambda samples, fn=Tuple(
        Pad(axis=0, pad_val=pad_token_id),  # input_ids
        Stack(dtype="int64")  # label
    ): [data for data in fn(samples)]

    dataloader = paddle.io.DataLoader(
        dataset,
        batch_sampler=sampler,
        return_list=True,
        collate_fn=batchify_fn)
    return dataloader

# from data import Tokenizer
tokenizer = Tokenizer()

# from paddlenlp.transformers import ErnieTokenizer
# tokenizer = ErnieTokenizer.from_pretrained('ernie-1.0')
tokenizer.set_vocab(vocab=token_embedding.vocab)

```

```python
# 给数据表中的所有商品名称进行编码，只做一次编码，提高计算速度
df["ids"]=df["title"].apply(lambda x:  paddle.to_tensor([tokenizer.text_to_ids(x)]).numpy() )
print(df.head())

```

```python

def list_gross(text_a,df):
    # 查看相似语句相关度
    # df表里有 Key,Title 两个字段

    # 去掉品牌信息，重点匹配其他信息
    text_a_ids = paddle.to_tensor([tokenizer.text_to_ids(text_a)])
    result_list=[]
    # 最后按照文本相似度进行计算
    for index,row in df.iterrows():
        text_b=row["title"]
        # 剔除品牌信息，因为前面已经进行过筛选，避免匹配不准确
        # text_b=clear_brand(text_b.replace(brand_cn.upper(), "").replace(brand_en.upper(), ""))
        # text_b_ids = paddle.to_tensor([tokenizer.text_to_ids(text_b)])
        text_b_ids =paddle.to_tensor(row["ids"]) 

        # print("text_a: {}".format(text_a))
        # print("text_b: {}".format(text_b))
        # print("text_b_key: {}".format(row["Key"]))
        sim_percent=model.get_cos_sim(text_a_ids, text_b_ids).numpy()[0]
        # print("cosine_sim: {}".format(sim_percent))
        # print("粗糙相似度: {} , {}".format(text_b,sim_percent))
        if sim_percent>=0.4:
            result_list.append([text_a,text_b,sim_percent])

    result_list = sorted(result_list, key=lambda item: -item[2])
    # print(result_list)
    # for list in result_list:
    # # for list in result_list[0:20]:
    #     print(list[1],list[2])
    return  result_list
    
    
```
 


### 5、精准算法
```python


# 精准相似度计算，调用百度的短文本相似度计算

similarity = Taskflow("text_similarity")

def list_accurate(similarity,percent_1):
    # [['口腔清洁护理品舌苔器', '舌苔清洁啫喱50G', 0.81275344], ['口腔清洁护理品舌苔器', '舌苔器', 0.8100314]]
    result_list = []
    for p in percent_1:
        # 精准计算相似度
        a=str(p[0])
        b=str(p[1])
        # a=a[:30]
        # b=b[:30]
        sim_percent = similarity([[a, b]])
        # print(sim_percent)
        sim_percent = sim_percent[0]["similarity"]
        # sim_percent =json.loads(sim_percent)["similarity"]
        # print(sim_percent)
        # sim_percent =sim_percent["similarity"]
        # print("cosine_sim: {}".format(sim_percent))
        if sim_percent > 0.5:
            result_list.append([p[0], p[1], sim_percent])

    result_list = sorted(result_list, key=lambda item: -item[2])
    # print(result_list)
    if len(result_list) > 0:
        print("最后的比对结果：")
        for list in result_list[0:20]:
            print(list[1], list[2])

        return result_list[0][1], result_list[0][2]
    else:
        print("没有找到任何可匹配的结果：")
        return '', 0
        
```

### 6、计算最相似的商品

```python


# 提取商品名称中的品牌名称
from pprint import pprint
from paddlenlp import Taskflow

# print(brand_list[:10])
def get_brand(title):
    # 品牌列表已经按照字符串长度从大到小排序，优先匹配字符串长的品牌
    for brand in brand_list:
        if title.find(brand)>=0:
            return brand

    # 如果在品牌列表中没有找到品牌，则使用百度NLP预测品牌信息 
    schema = ['品牌'] # Define the schema for entity extraction
    ie = Taskflow('information_extraction', schema=schema)
    brand=ie("蒙牛 特仑苏有机梦幻盖纯牛奶 250ml*10盒")
    pprint(brand)
    brand=brand[0]["品牌"][0]["text"]
    print(brand)

    if brand.index(" ")>0:
        print(brand.split(" ")[0])
    else:
        print(brand)
    
    return brand 

 

# 根据商品名称，找出最准确的sku
def get_sku(brand,title,df):
    # key = input()
    print("你输入的是:", title)
    # 品牌筛选会大大提高计算速度
    df=df[df["brand"].str.contains(brand)]
    df_temp = df[df["title"].str.contains(title, regex=False)]  # 避免正则表达式，比如碰到+，加号
    if not df_temp.empty:
        print("直接找到刚好匹配的记录")
        # 余弦相似度
        df_temp["percent"] = df_temp["title"].apply(
            lambda x: model.get_cos_sim(paddle.to_tensor([tokenizer.text_to_ids(x)]),
                                        paddle.to_tensor(
                                            [tokenizer.text_to_ids(title.upper())])).numpy()[0])
        df_temp.sort_values(by=["percent"], ascending=False, inplace=True)

        # 取第一个，最相似的记录
        sku = df_temp["sku"].iloc[0]
        ori_title = df_temp["title"].iloc[0]
    else:
        # 对全表进行余弦相似度计算  
        print("对全表进行余弦相似度计算")      
        percent_list = list_gross(title.upper(), df)

        print("粗糙比对后结果:")
        print(percent_list[:10])

        # 精准比对结果
        if len(percent_list) > 0:
            print("粗排找到{}个相似".format(len(percent_list)))
            if abs(percent_list[0][2] - 1) <= 0.01:  # 粗排已经很准了，就不需要精准排序了
                # return percent_list[0][1], 1
                print("粗排已经很准了，就不需要精准排序了")
                text, percent= percent_list[0][1], 1
            else:
                print("现在开始进行精准排序...")
                percent_list=percent_list[:10]
                text, percent = list_accurate(similarity, percent_list)
                # return text, percent
        else:
            print("不好意思，没有找到可匹配的结果")
            # return '', 0
            text=''
            percent=0

        print("精准匹配结果:")
        print(text, percent)

        # 根据名称找出精准的sku
        title=text
        if len(title) > 0:
            # key_df=df[  df["title_new"].str.contains(title) & df["brand_cn"].str.contains(brand_cn) ]
 
            # 重算相似度，解决有些名字长，刚好包含了短名称的问题
            df["percent"] = df["title"].apply(
                lambda x: model.get_cos_sim(paddle.to_tensor([tokenizer.text_to_ids(x)]),
                                            paddle.to_tensor(
                                                [tokenizer.text_to_ids(title.upper())])).numpy()[0])
            df.sort_values(by=["percent"], ascending=False, inplace=True)

            print("配对结果:")
            print(title)
            print(df.head(5).to_markdown())
            if not df.empty:
                ori_title = df["title"].iloc[0]
                # default_code = key_df["内部参考"].iloc[0]
                sku = df["sku"].iloc[0]
            else:
                ori_title = ""
                # default_code = ""
                sku = ""

            # print("最后的结果:{},{},{},{}".format(default_code,title,key_df["title"].iloc[0],percent))
            print("最后的结果:",  sku, '原始：', ori_title, '清洗后:', title, percent)

        else:
            ori_title = ""
            default_code = ""
            sku = ""


    print("返回结果:")
    print( sku, ori_title  )
    return  sku ,ori_title 

```

### 7、调试

```python

paddle.disable_static()

if __name__ == '__main__':

    # title="复原乳牛奶125ml*24 旺仔"
    title="特仑苏有机梦幻盖纯牛奶 250ml*10盒 蒙牛 "
    brand=get_brand(title)
    text =get_sku(brand,title,df)
    print(text)
    
```


